{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoW5kQatKscJ"
   },
   "source": [
    "# Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For first run in jupyter run next comand:\n",
    "!pip install pandas nltk sklearn matplotlib gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: nltk in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: sklearn in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: matplotlib in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: gensim in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (4.2.0)\n",
      "Requirement already satisfied: pyLDAvis in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (3.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from pandas) (1.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from nltk) (4.63.1)\n",
      "Requirement already satisfied: click in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: joblib in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: scikit-learn in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from matplotlib) (4.31.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from matplotlib) (20.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from matplotlib) (9.1.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from gensim) (1.8.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from gensim) (6.0.0)\n",
      "Requirement already satisfied: future in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: numexpr in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: funcy in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from pyLDAvis) (1.17)\n",
      "Requirement already satisfied: setuptools in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from pyLDAvis) (61.3.0)\n",
      "Requirement already satisfied: jinja2 in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from pyLDAvis) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from jinja2->pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "\u001B[33mWARNING: You are using pip version 21.1.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/Users/marina.romashkova/.pyenv/versions/3.9.5/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\n",
      "/Users/marina.romashkova/.pyenv/versions/3.9.5/bin/python3\n",
      "Python 3.9.5\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install pandas nltk sklearn matplotlib gensim pyLDAvis\n",
    "\n",
    "!which python3\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "us9ig1O_jJOH",
    "outputId": "42358ab6-5796-4819-f4f1-b6ef822fd5d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/marina.romashkova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/marina.romashkova/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/marina.romashkova/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/marina.romashkova/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marina.romashkova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from gensim.models import word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XauWSHBpifYX"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKiVE93uu16j"
   },
   "source": [
    "### Make dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZDmeLbnxK4KL"
   },
   "outputs": [],
   "source": [
    "df_neg = pd.read_csv(\"./data/processedNegative.csv\", header=None, sep=\",\")\n",
    "df_neg = df_neg.T.rename(columns={0:'tweet'})\n",
    "df_neg['tone'] = \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "UsDClki8K-1G",
    "outputId": "62544d2b-5b33-43a7-fc09-9c985e589ff3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet      tone\n",
       "0              How unhappy  some dogs like it though  negative\n",
       "1  talking to my over driver about where I'm goin...  negative\n",
       "2  Does anybody know if the Rand's likely to fall...  negative\n",
       "3         I miss going to gigs in Liverpool unhappy   negative\n",
       "4      There isnt a new Riverdale tonight ? unhappy   negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wRcy5XZwLCk5"
   },
   "outputs": [],
   "source": [
    "df_pos = pd.read_csv(\"./data/processedPositive.csv\", header=None, sep=\",\")\n",
    "df_pos = df_pos.T.rename(columns={0:'tweet'})\n",
    "df_pos['tone'] = \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ClUEaaLccupN",
    "outputId": "12a85dcc-b5ad-43c8-9c1a-6e4369c84d47"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An inspiration in all aspects: Fashion</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fitness</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beauty and personality. :)KISSES TheFashionIcon</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apka Apna Awam Ka Channel Frankline Tv Aam Adm...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beautiful album from  the greatest unsung guit...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet      tone\n",
       "0             An inspiration in all aspects: Fashion  positive\n",
       "1                                            fitness  positive\n",
       "2    beauty and personality. :)KISSES TheFashionIcon  positive\n",
       "3  Apka Apna Awam Ka Channel Frankline Tv Aam Adm...  positive\n",
       "4  Beautiful album from  the greatest unsung guit...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0YheRKlRd72c"
   },
   "outputs": [],
   "source": [
    "df_neut = pd.read_csv(\"./data/processedNeutral.csv\", header=None)\n",
    "df_neut = df_neut.T.rename(columns={0:'tweet'})\n",
    "df_neut['tone'] = \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ozZANOqXd742",
    "outputId": "2cdce2aa-ff63-4ce5-a0f9-e221cac69ede"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pak PM survives removal scare</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>but court orders further probe into corruptio...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Supreme Court quashes criminal complaint again...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Art of Living's fights back over Yamuna floodp...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>livid.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet     tone\n",
       "0                      Pak PM survives removal scare  neutral\n",
       "1   but court orders further probe into corruptio...  neutral\n",
       "2  Supreme Court quashes criminal complaint again...  neutral\n",
       "3  Art of Living's fights back over Yamuna floodp...  neutral\n",
       "4                                            livid.   neutral"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LpbhF_Q7eA8c"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_neg, df_pos, df_neut], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "7Uf83zZ5eA_K",
    "outputId": "fe12f449-a46f-4b1e-fa4a-ef656c3cd450"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>IDFC official Vikram Limaye</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>former captain Diana Edulji are others in pan...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet      tone\n",
       "0                 How unhappy  some dogs like it though  negative\n",
       "1     talking to my over driver about where I'm goin...  negative\n",
       "2     Does anybody know if the Rand's likely to fall...  negative\n",
       "3            I miss going to gigs in Liverpool unhappy   negative\n",
       "4         There isnt a new Riverdale tonight ? unhappy   negative\n",
       "...                                                 ...       ...\n",
       "3868                        IDFC official Vikram Limaye   neutral\n",
       "3869   former captain Diana Edulji are others in pan...   neutral\n",
       "3870  Supreme Court names former CAG as head of 4-me...   neutral\n",
       "3871  Court summons CM suspended BJP MP as accused i...   neutral\n",
       "3872  Amulya Patnaik has been appointed new Delhi po...   neutral\n",
       "\n",
       "[3873 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "-6Oscku1RPMP",
    "outputId": "65e10700-099f-4776-947c-11cf49088abf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['tone'].value_counts().plot(kind=\"bar\", rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1pRiPwaicyy"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization - split tweets into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XgEE-ir5eBE2"
   },
   "outputs": [],
   "source": [
    "df['tokenization'] = df.apply(lambda row: nltk.word_tokenize(str(row['tweet']).lower()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "xQEnrgXveBHt",
    "outputId": "ef378c7d-18c6-4dcd-ef91-0efee09c3ee8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "      <th>tokenization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>negative</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>IDFC official Vikram Limaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>former captain Diana Edulji are others in pan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[supreme, court, names, former, cag, as, head,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, as, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[amulya, patnaik, has, been, appointed, new, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet      tone  \\\n",
       "0                 How unhappy  some dogs like it though  negative   \n",
       "1     talking to my over driver about where I'm goin...  negative   \n",
       "2     Does anybody know if the Rand's likely to fall...  negative   \n",
       "3            I miss going to gigs in Liverpool unhappy   negative   \n",
       "4         There isnt a new Riverdale tonight ? unhappy   negative   \n",
       "...                                                 ...       ...   \n",
       "3868                        IDFC official Vikram Limaye   neutral   \n",
       "3869   former captain Diana Edulji are others in pan...   neutral   \n",
       "3870  Supreme Court names former CAG as head of 4-me...   neutral   \n",
       "3871  Court summons CM suspended BJP MP as accused i...   neutral   \n",
       "3872  Amulya Patnaik has been appointed new Delhi po...   neutral   \n",
       "\n",
       "                                           tokenization  \n",
       "0          [how, unhappy, some, dogs, like, it, though]  \n",
       "1     [talking, to, my, over, driver, about, where, ...  \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...  \n",
       "3     [i, miss, going, to, gigs, in, liverpool, unha...  \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...  \n",
       "...                                                 ...  \n",
       "3868                   [idfc, official, vikram, limaye]  \n",
       "3869  [former, captain, diana, edulji, are, others, ...  \n",
       "3870  [supreme, court, names, former, cag, as, head,...  \n",
       "3871  [court, summons, cm, suspended, bjp, mp, as, a...  \n",
       "3872  [amulya, patnaik, has, been, appointed, new, d...  \n",
       "\n",
       "[3873 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "stemming - finding the stem of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "tPJcgxdUeBLA",
    "outputId": "d37bb0e4-52d5-4c1e-a277-b0947590850b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>stemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>negative</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>IDFC official Vikram Limaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>former captain Diana Edulji are others in pan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[supreme, court, names, former, cag, as, head,...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, as, a...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[amulya, patnaik, has, been, appointed, new, d...</td>\n",
       "      <td>[amulya, patnaik, has, been, appoint, new, del...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet      tone  \\\n",
       "0                 How unhappy  some dogs like it though  negative   \n",
       "1     talking to my over driver about where I'm goin...  negative   \n",
       "2     Does anybody know if the Rand's likely to fall...  negative   \n",
       "3            I miss going to gigs in Liverpool unhappy   negative   \n",
       "4         There isnt a new Riverdale tonight ? unhappy   negative   \n",
       "...                                                 ...       ...   \n",
       "3868                        IDFC official Vikram Limaye   neutral   \n",
       "3869   former captain Diana Edulji are others in pan...   neutral   \n",
       "3870  Supreme Court names former CAG as head of 4-me...   neutral   \n",
       "3871  Court summons CM suspended BJP MP as accused i...   neutral   \n",
       "3872  Amulya Patnaik has been appointed new Delhi po...   neutral   \n",
       "\n",
       "                                           tokenization  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3     [i, miss, going, to, gigs, in, liverpool, unha...   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, names, former, cag, as, head,...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, as, a...   \n",
       "3872  [amulya, patnaik, has, been, appointed, new, d...   \n",
       "\n",
       "                                               stemming  \n",
       "0           [how, unhappi, some, dog, like, it, though]  \n",
       "1     [talk, to, my, over, driver, about, where, i, ...  \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...  \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]  \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...  \n",
       "...                                                 ...  \n",
       "3868                      [idfc, offici, vikram, limay]  \n",
       "3869  [former, captain, diana, edulji, are, other, i...  \n",
       "3870  [suprem, court, name, former, cag, as, head, o...  \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...  \n",
       "3872  [amulya, patnaik, has, been, appoint, new, del...  \n",
       "\n",
       "[3873 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer(language=\"english\")\n",
    "df['stemming'] = df['tokenization'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "CIwKzic7v5US",
    "outputId": "170e5a2c-8f34-48c4-ab7f-0a6f356ce959"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>stemming</th>\n",
       "      <th>stemming+</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>negative</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>IDFC official Vikram Limaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>former captain Diana Edulji are others in pan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[supreme, court, names, former, cag, as, head,...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, as, a...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[amulya, patnaik, has, been, appointed, new, d...</td>\n",
       "      <td>[amulya, patnaik, has, been, appoint, new, del...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appoint, new, delh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet      tone  \\\n",
       "0                 How unhappy  some dogs like it though  negative   \n",
       "1     talking to my over driver about where I'm goin...  negative   \n",
       "2     Does anybody know if the Rand's likely to fall...  negative   \n",
       "3            I miss going to gigs in Liverpool unhappy   negative   \n",
       "4         There isnt a new Riverdale tonight ? unhappy   negative   \n",
       "...                                                 ...       ...   \n",
       "3868                        IDFC official Vikram Limaye   neutral   \n",
       "3869   former captain Diana Edulji are others in pan...   neutral   \n",
       "3870  Supreme Court names former CAG as head of 4-me...   neutral   \n",
       "3871  Court summons CM suspended BJP MP as accused i...   neutral   \n",
       "3872  Amulya Patnaik has been appointed new Delhi po...   neutral   \n",
       "\n",
       "                                           tokenization  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3     [i, miss, going, to, gigs, in, liverpool, unha...   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, names, former, cag, as, head,...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, as, a...   \n",
       "3872  [amulya, patnaik, has, been, appointed, new, d...   \n",
       "\n",
       "                                               stemming  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, has, been, appoint, new, del...   \n",
       "\n",
       "                                              stemming+  \n",
       "0           [how, unhappi, some, dog, like, it, though]  \n",
       "1     [talk, to, my, over, driver, about, where, i, ...  \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...  \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]  \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...  \n",
       "...                                                 ...  \n",
       "3868                      [idfc, offici, vikram, limay]  \n",
       "3869  [former, captain, diana, edulji, are, other, i...  \n",
       "3870  [suprem, court, name, former, cag, as, head, o...  \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...  \n",
       "3872  [amulya, patnaik, ha, been, appoint, new, delh...  \n",
       "\n",
       "[3873 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer_ = nltk.stem.PorterStemmer()\n",
    "df['stemming+'] = df['tokenization'].apply(lambda x: [stemmer_.stem(y) for y in x])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "lemmatization - finding the root of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "IK6baJaiv5XR",
    "outputId": "48f23036-64ff-4846-b9e2-9a58da384ae9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>stemming</th>\n",
       "      <th>stemming+</th>\n",
       "      <th>lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>negative</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liverpool, unhappy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>IDFC official Vikram Limaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>former captain Diana Edulji are others in pan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[supreme, court, names, former, cag, as, head,...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, as, a...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, a, ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[amulya, patnaik, has, been, appointed, new, d...</td>\n",
       "      <td>[amulya, patnaik, has, been, appoint, new, del...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appoint, new, delh...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appointed, new, de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet      tone  \\\n",
       "0                 How unhappy  some dogs like it though  negative   \n",
       "1     talking to my over driver about where I'm goin...  negative   \n",
       "2     Does anybody know if the Rand's likely to fall...  negative   \n",
       "3            I miss going to gigs in Liverpool unhappy   negative   \n",
       "4         There isnt a new Riverdale tonight ? unhappy   negative   \n",
       "...                                                 ...       ...   \n",
       "3868                        IDFC official Vikram Limaye   neutral   \n",
       "3869   former captain Diana Edulji are others in pan...   neutral   \n",
       "3870  Supreme Court names former CAG as head of 4-me...   neutral   \n",
       "3871  Court summons CM suspended BJP MP as accused i...   neutral   \n",
       "3872  Amulya Patnaik has been appointed new Delhi po...   neutral   \n",
       "\n",
       "                                           tokenization  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3     [i, miss, going, to, gigs, in, liverpool, unha...   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, names, former, cag, as, head,...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, as, a...   \n",
       "3872  [amulya, patnaik, has, been, appointed, new, d...   \n",
       "\n",
       "                                               stemming  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, has, been, appoint, new, del...   \n",
       "\n",
       "                                              stemming+  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, ha, been, appoint, new, delh...   \n",
       "\n",
       "                                          lemmatization  \n",
       "0           [how, unhappy, some, dog, like, it, though]  \n",
       "1     [talking, to, my, over, driver, about, where, ...  \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...  \n",
       "3     [i, miss, going, to, gig, in, liverpool, unhappy]  \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...  \n",
       "...                                                 ...  \n",
       "3868                   [idfc, official, vikram, limaye]  \n",
       "3869  [former, captain, diana, edulji, are, others, ...  \n",
       "3870  [supreme, court, name, former, cag, a, head, o...  \n",
       "3871  [court, summons, cm, suspended, bjp, mp, a, ac...  \n",
       "3872  [amulya, patnaik, ha, been, appointed, new, de...  \n",
       "\n",
       "[3873 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "df['lemmatization'] = df['tokenization'].apply(lambda x: [lemmatizer.lemmatize(str(y)) for y in x])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "misspealing - correct possible misspealings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seiUlvrtv5d_",
    "outputId": "a326199d-e82d-455f-fac9-38dc1f0cb7fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"amzing\" is a misspealing of word - \"amazing\"\n"
     ]
    }
   ],
   "source": [
    "correct_words = words.words()\n",
    "\n",
    "def misspealings(word):\n",
    "  temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
    "                              set(ngrams(w, 2))),w)\n",
    "            for w in correct_words if w[0]==word[0]]\n",
    "  return sorted(temp, key = lambda val:val[0])[0][1]\n",
    "\n",
    "sample_word = \"amzing\"\n",
    "print(f'The word \"{sample_word}\" is a misspealing of word - \"{misspealings(sample_word)}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_misspeal(word):\n",
    "    if len(word) > 2 and \"'\" not in word and \".\" not in word and word.isalpha():\n",
    "        return misspealings(word)\n",
    "    return word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "GoTnJj9uv5gp",
    "outputId": "1338b27c-3833-4614-8d60-698793d035cd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>stemming</th>\n",
       "      <th>stemming+</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>misspealings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>negative</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liverpool, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>IDFC official Vikram Limaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>former captain Diana Edulji are others in pan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[supreme, court, names, former, cag, as, head,...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, as, head, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, as, a...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, a, ac...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[amulya, patnaik, has, been, appointed, new, d...</td>\n",
       "      <td>[amulya, patnaik, has, been, appoint, new, del...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appoint, new, delh...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appointed, new, de...</td>\n",
       "      <td>[amula, paik, hash, been, appointe, new, dele,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet      tone  \\\n",
       "0                 How unhappy  some dogs like it though  negative   \n",
       "1     talking to my over driver about where I'm goin...  negative   \n",
       "2     Does anybody know if the Rand's likely to fall...  negative   \n",
       "3            I miss going to gigs in Liverpool unhappy   negative   \n",
       "4         There isnt a new Riverdale tonight ? unhappy   negative   \n",
       "...                                                 ...       ...   \n",
       "3868                        IDFC official Vikram Limaye   neutral   \n",
       "3869   former captain Diana Edulji are others in pan...   neutral   \n",
       "3870  Supreme Court names former CAG as head of 4-me...   neutral   \n",
       "3871  Court summons CM suspended BJP MP as accused i...   neutral   \n",
       "3872  Amulya Patnaik has been appointed new Delhi po...   neutral   \n",
       "\n",
       "                                           tokenization  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3     [i, miss, going, to, gigs, in, liverpool, unha...   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, names, former, cag, as, head,...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, as, a...   \n",
       "3872  [amulya, patnaik, has, been, appointed, new, d...   \n",
       "\n",
       "                                               stemming  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, has, been, appoint, new, del...   \n",
       "\n",
       "                                              stemming+  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, ha, been, appoint, new, delh...   \n",
       "\n",
       "                                          lemmatization  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3     [i, miss, going, to, gig, in, liverpool, unhappy]   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, a, ac...   \n",
       "3872  [amulya, patnaik, ha, been, appointed, new, de...   \n",
       "\n",
       "                                           misspealings  \n",
       "0          [how, unhappy, some, dogs, like, it, though]  \n",
       "1     [talking, to, my, over, driver, about, where, ...  \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...  \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]  \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...  \n",
       "...                                                 ...  \n",
       "3868                     [id, official, viagram, liman]  \n",
       "3869  [former, captain, dian, educe, are, other, in,...  \n",
       "3870  [supreme, court, name, former, cag, as, head, ...  \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...  \n",
       "3872  [amula, paik, hash, been, appointe, new, dele,...  \n",
       "\n",
       "[3873 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['misspealings'] = df['tokenization'].apply(lambda x: [check_misspeal(str(y)) for y in x])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>stemming</th>\n",
       "      <th>stemming+</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>misspealings</th>\n",
       "      <th>misspealings+lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>negative</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liverpool, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>IDFC official Vikram Limaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>former captain Diana Edulji are others in pan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[supreme, court, names, former, cag, as, head,...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, as, head, ...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, as, a...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, a, ac...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[amulya, patnaik, has, been, appointed, new, d...</td>\n",
       "      <td>[amulya, patnaik, has, been, appoint, new, del...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appoint, new, delh...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appointed, new, de...</td>\n",
       "      <td>[amula, paik, hash, been, appointe, new, dele,...</td>\n",
       "      <td>[amula, paik, ha, been, appointe, new, dele, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet      tone  \\\n",
       "0                 How unhappy  some dogs like it though  negative   \n",
       "1     talking to my over driver about where I'm goin...  negative   \n",
       "2     Does anybody know if the Rand's likely to fall...  negative   \n",
       "3            I miss going to gigs in Liverpool unhappy   negative   \n",
       "4         There isnt a new Riverdale tonight ? unhappy   negative   \n",
       "...                                                 ...       ...   \n",
       "3868                        IDFC official Vikram Limaye   neutral   \n",
       "3869   former captain Diana Edulji are others in pan...   neutral   \n",
       "3870  Supreme Court names former CAG as head of 4-me...   neutral   \n",
       "3871  Court summons CM suspended BJP MP as accused i...   neutral   \n",
       "3872  Amulya Patnaik has been appointed new Delhi po...   neutral   \n",
       "\n",
       "                                           tokenization  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3     [i, miss, going, to, gigs, in, liverpool, unha...   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, names, former, cag, as, head,...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, as, a...   \n",
       "3872  [amulya, patnaik, has, been, appointed, new, d...   \n",
       "\n",
       "                                               stemming  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, has, been, appoint, new, del...   \n",
       "\n",
       "                                              stemming+  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, ha, been, appoint, new, delh...   \n",
       "\n",
       "                                          lemmatization  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3     [i, miss, going, to, gig, in, liverpool, unhappy]   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, a, ac...   \n",
       "3872  [amulya, patnaik, ha, been, appointed, new, de...   \n",
       "\n",
       "                                           misspealings  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, as, head, ...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, hash, been, appointe, new, dele,...   \n",
       "\n",
       "                                     misspealings+lemma  \n",
       "0           [how, unhappy, some, dog, like, it, though]  \n",
       "1     [talking, to, my, over, driver, about, where, ...  \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...  \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]  \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...  \n",
       "...                                                 ...  \n",
       "3868                     [id, official, viagram, liman]  \n",
       "3869  [former, captain, dian, educe, are, other, in,...  \n",
       "3870  [supreme, court, name, former, cag, a, head, o...  \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...  \n",
       "3872  [amula, paik, ha, been, appointe, new, dele, p...  \n",
       "\n",
       "[3873 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['misspealings+lemma'] = df['lemmatization'].apply(lambda x: [check_misspeal(str(y)) for y in x])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "otr1ijKLv5kR",
    "outputId": "9178092a-670b-4127-997f-72da606a4ad3"
   },
   "source": [
    "Filtering tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XDHc3LQKv5mT",
    "outputId": "58194df3-4e3a-4dc8-aa06-b3b8b31a1e50",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence \"This is a sample sentence,\n",
      "                  showing off the stop words filtration\" was filtered:\n",
      " \"['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration']\"\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "example_sent = \"\"\"This is a sample sentence,\n",
    "                  showing off the stop words filtration\"\"\"\n",
    "word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "print(f'The sentence \"{example_sent}\" was filtered:\\n \"{filtered_sentence}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 798
    },
    "id": "CR_MSRuZjarZ",
    "outputId": "5db3a458-9dc4-43ae-a135-90cc2d74c13b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>stemming</th>\n",
       "      <th>stemming+</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>misspealings</th>\n",
       "      <th>misspealings+lemma</th>\n",
       "      <th>filtered+tokenization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>negative</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[unhappy, dogs, like, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, driver, 'm, goinghe, said, 'd, love,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[anybody, know, rand, 's, likely, fall, dollar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liverpool, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[miss, going, gigs, liverpool, unhappy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[isnt, new, riverdale, tonight, ?, unhappy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>IDFC official Vikram Limaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>former captain Diana Edulji are others in pan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, diana, edulji, others, panel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[supreme, court, names, former, cag, as, head,...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, as, head, ...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, names, former, cag, head, 4-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, as, a...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, a, ac...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, accus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[amulya, patnaik, has, been, appointed, new, d...</td>\n",
       "      <td>[amulya, patnaik, has, been, appoint, new, del...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appoint, new, delh...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appointed, new, de...</td>\n",
       "      <td>[amula, paik, hash, been, appointe, new, dele,...</td>\n",
       "      <td>[amula, paik, ha, been, appointe, new, dele, p...</td>\n",
       "      <td>[amulya, patnaik, appointed, new, delhi, polic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet      tone  \\\n",
       "0                 How unhappy  some dogs like it though  negative   \n",
       "1     talking to my over driver about where I'm goin...  negative   \n",
       "2     Does anybody know if the Rand's likely to fall...  negative   \n",
       "3            I miss going to gigs in Liverpool unhappy   negative   \n",
       "4         There isnt a new Riverdale tonight ? unhappy   negative   \n",
       "...                                                 ...       ...   \n",
       "3868                        IDFC official Vikram Limaye   neutral   \n",
       "3869   former captain Diana Edulji are others in pan...   neutral   \n",
       "3870  Supreme Court names former CAG as head of 4-me...   neutral   \n",
       "3871  Court summons CM suspended BJP MP as accused i...   neutral   \n",
       "3872  Amulya Patnaik has been appointed new Delhi po...   neutral   \n",
       "\n",
       "                                           tokenization  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3     [i, miss, going, to, gigs, in, liverpool, unha...   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, names, former, cag, as, head,...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, as, a...   \n",
       "3872  [amulya, patnaik, has, been, appointed, new, d...   \n",
       "\n",
       "                                               stemming  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, has, been, appoint, new, del...   \n",
       "\n",
       "                                              stemming+  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, ha, been, appoint, new, delh...   \n",
       "\n",
       "                                          lemmatization  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3     [i, miss, going, to, gig, in, liverpool, unhappy]   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, a, ac...   \n",
       "3872  [amulya, patnaik, ha, been, appointed, new, de...   \n",
       "\n",
       "                                           misspealings  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, as, head, ...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, hash, been, appointe, new, dele,...   \n",
       "\n",
       "                                     misspealings+lemma  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, ha, been, appointe, new, dele, p...   \n",
       "\n",
       "                                  filtered+tokenization  \n",
       "0                         [unhappy, dogs, like, though]  \n",
       "1     [talking, driver, 'm, goinghe, said, 'd, love,...  \n",
       "2     [anybody, know, rand, 's, likely, fall, dollar...  \n",
       "3               [miss, going, gigs, liverpool, unhappy]  \n",
       "4           [isnt, new, riverdale, tonight, ?, unhappy]  \n",
       "...                                                 ...  \n",
       "3868                   [idfc, official, vikram, limaye]  \n",
       "3869  [former, captain, diana, edulji, others, panel...  \n",
       "3870  [supreme, court, names, former, cag, head, 4-m...  \n",
       "3871  [court, summons, cm, suspended, bjp, mp, accus...  \n",
       "3872  [amulya, patnaik, appointed, new, delhi, polic...  \n",
       "\n",
       "[3873 rows x 9 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['filtered+tokenization'] = df['tokenization'].apply(lambda x: [y for y in x if not y.lower() in stop_words])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 919
    },
    "id": "oSOFZ0X5jbD_",
    "outputId": "ecee8673-c007-4525-9e71-a6318f482931"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>stemming</th>\n",
       "      <th>stemming+</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>misspealings</th>\n",
       "      <th>misspealings+lemma</th>\n",
       "      <th>filtered+tokenization</th>\n",
       "      <th>misspealings+lemma+filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>negative</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[unhappy, dogs, like, though]</td>\n",
       "      <td>[unhappy, dog, like, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, driver, 'm, goinghe, said, 'd, love,...</td>\n",
       "      <td>[talking, driver, 'm, going, said, 'd, love, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[anybody, know, rand, 's, likely, fall, dollar...</td>\n",
       "      <td>[doe, anybody, know, rand, 's, likely, fall, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liverpool, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[miss, going, gigs, liverpool, unhappy]</td>\n",
       "      <td>[miss, going, gig, liver, unhappy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[isnt, new, riverdale, tonight, ?, unhappy]</td>\n",
       "      <td>[new, riverdamp, tonight, ?, unhappy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>IDFC official Vikram Limaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>former captain Diana Edulji are others in pan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, diana, edulji, others, panel...</td>\n",
       "      <td>[former, captain, dian, educe, panel, run]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[supreme, court, names, former, cag, as, head,...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, as, head, ...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, names, former, cag, head, 4-m...</td>\n",
       "      <td>[supreme, court, name, former, cag, head, 4-me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, as, a...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, a, ac...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, accus...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[amulya, patnaik, has, been, appointed, new, d...</td>\n",
       "      <td>[amulya, patnaik, has, been, appoint, new, del...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appoint, new, delh...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appointed, new, de...</td>\n",
       "      <td>[amula, paik, hash, been, appointe, new, dele,...</td>\n",
       "      <td>[amula, paik, ha, been, appointe, new, dele, p...</td>\n",
       "      <td>[amulya, patnaik, appointed, new, delhi, polic...</td>\n",
       "      <td>[amula, paik, ha, appointe, new, dele, police,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet      tone  \\\n",
       "0                 How unhappy  some dogs like it though  negative   \n",
       "1     talking to my over driver about where I'm goin...  negative   \n",
       "2     Does anybody know if the Rand's likely to fall...  negative   \n",
       "3            I miss going to gigs in Liverpool unhappy   negative   \n",
       "4         There isnt a new Riverdale tonight ? unhappy   negative   \n",
       "...                                                 ...       ...   \n",
       "3868                        IDFC official Vikram Limaye   neutral   \n",
       "3869   former captain Diana Edulji are others in pan...   neutral   \n",
       "3870  Supreme Court names former CAG as head of 4-me...   neutral   \n",
       "3871  Court summons CM suspended BJP MP as accused i...   neutral   \n",
       "3872  Amulya Patnaik has been appointed new Delhi po...   neutral   \n",
       "\n",
       "                                           tokenization  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3     [i, miss, going, to, gigs, in, liverpool, unha...   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, names, former, cag, as, head,...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, as, a...   \n",
       "3872  [amulya, patnaik, has, been, appointed, new, d...   \n",
       "\n",
       "                                               stemming  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, has, been, appoint, new, del...   \n",
       "\n",
       "                                              stemming+  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, ha, been, appoint, new, delh...   \n",
       "\n",
       "                                          lemmatization  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3     [i, miss, going, to, gig, in, liverpool, unhappy]   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, a, ac...   \n",
       "3872  [amulya, patnaik, ha, been, appointed, new, de...   \n",
       "\n",
       "                                           misspealings  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, as, head, ...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, hash, been, appointe, new, dele,...   \n",
       "\n",
       "                                     misspealings+lemma  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, ha, been, appointe, new, dele, p...   \n",
       "\n",
       "                                  filtered+tokenization  \\\n",
       "0                         [unhappy, dogs, like, though]   \n",
       "1     [talking, driver, 'm, goinghe, said, 'd, love,...   \n",
       "2     [anybody, know, rand, 's, likely, fall, dollar...   \n",
       "3               [miss, going, gigs, liverpool, unhappy]   \n",
       "4           [isnt, new, riverdale, tonight, ?, unhappy]   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, others, panel...   \n",
       "3870  [supreme, court, names, former, cag, head, 4-m...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, accus...   \n",
       "3872  [amulya, patnaik, appointed, new, delhi, polic...   \n",
       "\n",
       "                            misspealings+lemma+filtered  \n",
       "0                          [unhappy, dog, like, though]  \n",
       "1     [talking, driver, 'm, going, said, 'd, love, g...  \n",
       "2     [doe, anybody, know, rand, 's, likely, fall, d...  \n",
       "3                    [miss, going, gig, liver, unhappy]  \n",
       "4                 [new, riverdamp, tonight, ?, unhappy]  \n",
       "...                                                 ...  \n",
       "3868                     [id, official, viagram, liman]  \n",
       "3869         [former, captain, dian, educe, panel, run]  \n",
       "3870  [supreme, court, name, former, cag, head, 4-me...  \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...  \n",
       "3872  [amula, paik, ha, appointe, new, dele, police,...  \n",
       "\n",
       "[3873 rows x 10 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['misspealings+lemma+filtered'] = df['misspealings+lemma'].apply(lambda x: [y for y in x if not y.lower() in stop_words])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zF4aHYYjPMWD"
   },
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgQc-6_kzOKM"
   },
   "source": [
    "### OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AASpM5XF7WRV",
    "outputId": "febf0cc5-d5ab-4dfb-d0ff-206c463e5b9e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>stemming</th>\n",
       "      <th>stemming+</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>misspealings</th>\n",
       "      <th>misspealings+lemma</th>\n",
       "      <th>filtered+tokenization</th>\n",
       "      <th>misspealings+lemma+filtered</th>\n",
       "      <th>ohe_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>negative</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[unhappy, dogs, like, though]</td>\n",
       "      <td>[unhappy, dog, like, though]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, driver, 'm, goinghe, said, 'd, love,...</td>\n",
       "      <td>[talking, driver, 'm, going, said, 'd, love, g...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[anybody, know, rand, 's, likely, fall, dollar...</td>\n",
       "      <td>[doe, anybody, know, rand, 's, likely, fall, d...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liverpool, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[miss, going, gigs, liverpool, unhappy]</td>\n",
       "      <td>[miss, going, gig, liver, unhappy]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[isnt, new, riverdale, tonight, ?, unhappy]</td>\n",
       "      <td>[new, riverdamp, tonight, ?, unhappy]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>IDFC official Vikram Limaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>former captain Diana Edulji are others in pan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, diana, edulji, others, panel...</td>\n",
       "      <td>[former, captain, dian, educe, panel, run]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[supreme, court, names, former, cag, as, head,...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, as, head, ...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, names, former, cag, head, 4-m...</td>\n",
       "      <td>[supreme, court, name, former, cag, head, 4-me...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, as, a...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, a, ac...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, accus...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[amulya, patnaik, has, been, appointed, new, d...</td>\n",
       "      <td>[amulya, patnaik, has, been, appoint, new, del...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appoint, new, delh...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appointed, new, de...</td>\n",
       "      <td>[amula, paik, hash, been, appointe, new, dele,...</td>\n",
       "      <td>[amula, paik, ha, been, appointe, new, dele, p...</td>\n",
       "      <td>[amulya, patnaik, appointed, new, delhi, polic...</td>\n",
       "      <td>[amula, paik, ha, appointe, new, dele, police,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet      tone  \\\n",
       "0                 How unhappy  some dogs like it though  negative   \n",
       "1     talking to my over driver about where I'm goin...  negative   \n",
       "2     Does anybody know if the Rand's likely to fall...  negative   \n",
       "3            I miss going to gigs in Liverpool unhappy   negative   \n",
       "4         There isnt a new Riverdale tonight ? unhappy   negative   \n",
       "...                                                 ...       ...   \n",
       "3868                        IDFC official Vikram Limaye   neutral   \n",
       "3869   former captain Diana Edulji are others in pan...   neutral   \n",
       "3870  Supreme Court names former CAG as head of 4-me...   neutral   \n",
       "3871  Court summons CM suspended BJP MP as accused i...   neutral   \n",
       "3872  Amulya Patnaik has been appointed new Delhi po...   neutral   \n",
       "\n",
       "                                           tokenization  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3     [i, miss, going, to, gigs, in, liverpool, unha...   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, names, former, cag, as, head,...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, as, a...   \n",
       "3872  [amulya, patnaik, has, been, appointed, new, d...   \n",
       "\n",
       "                                               stemming  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, has, been, appoint, new, del...   \n",
       "\n",
       "                                              stemming+  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, ha, been, appoint, new, delh...   \n",
       "\n",
       "                                          lemmatization  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3     [i, miss, going, to, gig, in, liverpool, unhappy]   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, a, ac...   \n",
       "3872  [amulya, patnaik, ha, been, appointed, new, de...   \n",
       "\n",
       "                                           misspealings  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, as, head, ...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, hash, been, appointe, new, dele,...   \n",
       "\n",
       "                                     misspealings+lemma  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, ha, been, appointe, new, dele, p...   \n",
       "\n",
       "                                  filtered+tokenization  \\\n",
       "0                         [unhappy, dogs, like, though]   \n",
       "1     [talking, driver, 'm, goinghe, said, 'd, love,...   \n",
       "2     [anybody, know, rand, 's, likely, fall, dollar...   \n",
       "3               [miss, going, gigs, liverpool, unhappy]   \n",
       "4           [isnt, new, riverdale, tonight, ?, unhappy]   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, others, panel...   \n",
       "3870  [supreme, court, names, former, cag, head, 4-m...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, accus...   \n",
       "3872  [amulya, patnaik, appointed, new, delhi, polic...   \n",
       "\n",
       "                            misspealings+lemma+filtered  \\\n",
       "0                          [unhappy, dog, like, though]   \n",
       "1     [talking, driver, 'm, going, said, 'd, love, g...   \n",
       "2     [doe, anybody, know, rand, 's, likely, fall, d...   \n",
       "3                    [miss, going, gig, liver, unhappy]   \n",
       "4                 [new, riverdamp, tonight, ?, unhappy]   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869         [former, captain, dian, educe, panel, run]   \n",
       "3870  [supreme, court, name, former, cag, head, 4-me...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, ha, appointe, new, dele, police,...   \n",
       "\n",
       "                                              ohe_tweet  \n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                 ...  \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[3873 rows x 11 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder(sparse=False)\n",
    "X_ohe = ohe.fit_transform((df[['tweet']]).astype('str'))\n",
    "df['ohe_tweet'] = list(X_ohe)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "GtpwwVVM53vM"
   },
   "outputs": [],
   "source": [
    "preprocessing_ways = [\n",
    "    \"tokenization\",\n",
    "    \"stemming\",\n",
    "    \"stemming+\",\n",
    "    \"lemmatization\",\n",
    "    \"misspealings\",\n",
    "    \"misspealings+lemma\",\n",
    "    \"filtered+tokenization\",\n",
    "    \"misspealings+lemma+filtered\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>stemming</th>\n",
       "      <th>stemming+</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>misspealings</th>\n",
       "      <th>misspealings+lemma</th>\n",
       "      <th>filtered+tokenization</th>\n",
       "      <th>misspealings+lemma+filtered</th>\n",
       "      <th>ohe_tweet</th>\n",
       "      <th>ohe_tokenization</th>\n",
       "      <th>ohe_stemming</th>\n",
       "      <th>ohe_stemming+</th>\n",
       "      <th>ohe_lemmatization</th>\n",
       "      <th>ohe_misspealings</th>\n",
       "      <th>ohe_misspealings+lemma</th>\n",
       "      <th>ohe_filtered+tokenization</th>\n",
       "      <th>ohe_misspealings+lemma+filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>negative</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[unhappy, dogs, like, though]</td>\n",
       "      <td>[unhappy, dog, like, though]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, driver, 'm, goinghe, said, 'd, love,...</td>\n",
       "      <td>[talking, driver, 'm, going, said, 'd, love, g...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[anybody, know, rand, 's, likely, fall, dollar...</td>\n",
       "      <td>[doe, anybody, know, rand, 's, likely, fall, d...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liverpool, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[miss, going, gigs, liverpool, unhappy]</td>\n",
       "      <td>[miss, going, gig, liver, unhappy]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[isnt, new, riverdale, tonight, ?, unhappy]</td>\n",
       "      <td>[new, riverdamp, tonight, ?, unhappy]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>IDFC official Vikram Limaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>former captain Diana Edulji are others in pan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, diana, edulji, others, panel...</td>\n",
       "      <td>[former, captain, dian, educe, panel, run]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[supreme, court, names, former, cag, as, head,...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, as, head, ...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, names, former, cag, head, 4-m...</td>\n",
       "      <td>[supreme, court, name, former, cag, head, 4-me...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, as, a...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, a, ac...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, accus...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[amulya, patnaik, has, been, appointed, new, d...</td>\n",
       "      <td>[amulya, patnaik, has, been, appoint, new, del...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appoint, new, delh...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appointed, new, de...</td>\n",
       "      <td>[amula, paik, hash, been, appointe, new, dele,...</td>\n",
       "      <td>[amula, paik, ha, been, appointe, new, dele, p...</td>\n",
       "      <td>[amulya, patnaik, appointed, new, delhi, polic...</td>\n",
       "      <td>[amula, paik, ha, appointe, new, dele, police,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet      tone  \\\n",
       "0                 How unhappy  some dogs like it though  negative   \n",
       "1     talking to my over driver about where I'm goin...  negative   \n",
       "2     Does anybody know if the Rand's likely to fall...  negative   \n",
       "3            I miss going to gigs in Liverpool unhappy   negative   \n",
       "4         There isnt a new Riverdale tonight ? unhappy   negative   \n",
       "...                                                 ...       ...   \n",
       "3868                        IDFC official Vikram Limaye   neutral   \n",
       "3869   former captain Diana Edulji are others in pan...   neutral   \n",
       "3870  Supreme Court names former CAG as head of 4-me...   neutral   \n",
       "3871  Court summons CM suspended BJP MP as accused i...   neutral   \n",
       "3872  Amulya Patnaik has been appointed new Delhi po...   neutral   \n",
       "\n",
       "                                           tokenization  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3     [i, miss, going, to, gigs, in, liverpool, unha...   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, names, former, cag, as, head,...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, as, a...   \n",
       "3872  [amulya, patnaik, has, been, appointed, new, d...   \n",
       "\n",
       "                                               stemming  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, has, been, appoint, new, del...   \n",
       "\n",
       "                                              stemming+  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, ha, been, appoint, new, delh...   \n",
       "\n",
       "                                          lemmatization  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3     [i, miss, going, to, gig, in, liverpool, unhappy]   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, a, ac...   \n",
       "3872  [amulya, patnaik, ha, been, appointed, new, de...   \n",
       "\n",
       "                                           misspealings  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, as, head, ...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, hash, been, appointe, new, dele,...   \n",
       "\n",
       "                                     misspealings+lemma  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, ha, been, appointe, new, dele, p...   \n",
       "\n",
       "                                  filtered+tokenization  \\\n",
       "0                         [unhappy, dogs, like, though]   \n",
       "1     [talking, driver, 'm, goinghe, said, 'd, love,...   \n",
       "2     [anybody, know, rand, 's, likely, fall, dollar...   \n",
       "3               [miss, going, gigs, liverpool, unhappy]   \n",
       "4           [isnt, new, riverdale, tonight, ?, unhappy]   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, others, panel...   \n",
       "3870  [supreme, court, names, former, cag, head, 4-m...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, accus...   \n",
       "3872  [amulya, patnaik, appointed, new, delhi, polic...   \n",
       "\n",
       "                            misspealings+lemma+filtered  \\\n",
       "0                          [unhappy, dog, like, though]   \n",
       "1     [talking, driver, 'm, going, said, 'd, love, g...   \n",
       "2     [doe, anybody, know, rand, 's, likely, fall, d...   \n",
       "3                    [miss, going, gig, liver, unhappy]   \n",
       "4                 [new, riverdamp, tonight, ?, unhappy]   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869         [former, captain, dian, educe, panel, run]   \n",
       "3870  [supreme, court, name, former, cag, head, 4-me...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, ha, appointe, new, dele, police,...   \n",
       "\n",
       "                                              ohe_tweet  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                       ohe_tokenization  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                           ohe_stemming  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                          ohe_stemming+  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                      ohe_lemmatization  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                       ohe_misspealings  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                 ohe_misspealings+lemma  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                              ohe_filtered+tokenization  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                        ohe_misspealings+lemma+filtered  \n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                 ...  \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[3873 rows x 19 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for way in preprocessing_ways:\n",
    "    X_ohe = ohe.fit_transform((df[[way]]).astype('str'))\n",
    "    df['ohe_' + way] = list(X_ohe)\n",
    "df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckN2SrfezY89"
   },
   "source": [
    "### WordCounts\n",
    "Convert a collection of text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>stemming</th>\n",
       "      <th>stemming+</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>misspealings</th>\n",
       "      <th>misspealings+lemma</th>\n",
       "      <th>filtered+tokenization</th>\n",
       "      <th>misspealings+lemma+filtered</th>\n",
       "      <th>ohe_tweet</th>\n",
       "      <th>ohe_tokenization</th>\n",
       "      <th>ohe_stemming</th>\n",
       "      <th>ohe_stemming+</th>\n",
       "      <th>ohe_lemmatization</th>\n",
       "      <th>ohe_misspealings</th>\n",
       "      <th>ohe_misspealings+lemma</th>\n",
       "      <th>ohe_filtered+tokenization</th>\n",
       "      <th>ohe_misspealings+lemma+filtered</th>\n",
       "      <th>wc_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>negative</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[unhappy, dogs, like, though]</td>\n",
       "      <td>[unhappy, dog, like, though]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, driver, 'm, goinghe, said, 'd, love,...</td>\n",
       "      <td>[talking, driver, 'm, going, said, 'd, love, g...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[anybody, know, rand, 's, likely, fall, dollar...</td>\n",
       "      <td>[doe, anybody, know, rand, 's, likely, fall, d...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liverpool, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[miss, going, gigs, liverpool, unhappy]</td>\n",
       "      <td>[miss, going, gig, liver, unhappy]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[isnt, new, riverdale, tonight, ?, unhappy]</td>\n",
       "      <td>[new, riverdamp, tonight, ?, unhappy]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>IDFC official Vikram Limaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>former captain Diana Edulji are others in pan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, diana, edulji, others, panel...</td>\n",
       "      <td>[former, captain, dian, educe, panel, run]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[supreme, court, names, former, cag, as, head,...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, as, head, ...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, names, former, cag, head, 4-m...</td>\n",
       "      <td>[supreme, court, name, former, cag, head, 4-me...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, as, a...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, a, ac...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, accus...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[amulya, patnaik, has, been, appointed, new, d...</td>\n",
       "      <td>[amulya, patnaik, has, been, appoint, new, del...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appoint, new, delh...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appointed, new, de...</td>\n",
       "      <td>[amula, paik, hash, been, appointe, new, dele,...</td>\n",
       "      <td>[amula, paik, ha, been, appointe, new, dele, p...</td>\n",
       "      <td>[amulya, patnaik, appointed, new, delhi, polic...</td>\n",
       "      <td>[amula, paik, ha, appointe, new, dele, police,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet      tone  \\\n",
       "0                 How unhappy  some dogs like it though  negative   \n",
       "1     talking to my over driver about where I'm goin...  negative   \n",
       "2     Does anybody know if the Rand's likely to fall...  negative   \n",
       "3            I miss going to gigs in Liverpool unhappy   negative   \n",
       "4         There isnt a new Riverdale tonight ? unhappy   negative   \n",
       "...                                                 ...       ...   \n",
       "3868                        IDFC official Vikram Limaye   neutral   \n",
       "3869   former captain Diana Edulji are others in pan...   neutral   \n",
       "3870  Supreme Court names former CAG as head of 4-me...   neutral   \n",
       "3871  Court summons CM suspended BJP MP as accused i...   neutral   \n",
       "3872  Amulya Patnaik has been appointed new Delhi po...   neutral   \n",
       "\n",
       "                                           tokenization  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3     [i, miss, going, to, gigs, in, liverpool, unha...   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, names, former, cag, as, head,...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, as, a...   \n",
       "3872  [amulya, patnaik, has, been, appointed, new, d...   \n",
       "\n",
       "                                               stemming  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, has, been, appoint, new, del...   \n",
       "\n",
       "                                              stemming+  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, ha, been, appoint, new, delh...   \n",
       "\n",
       "                                          lemmatization  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3     [i, miss, going, to, gig, in, liverpool, unhappy]   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, a, ac...   \n",
       "3872  [amulya, patnaik, ha, been, appointed, new, de...   \n",
       "\n",
       "                                           misspealings  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, as, head, ...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, hash, been, appointe, new, dele,...   \n",
       "\n",
       "                                     misspealings+lemma  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, ha, been, appointe, new, dele, p...   \n",
       "\n",
       "                                  filtered+tokenization  \\\n",
       "0                         [unhappy, dogs, like, though]   \n",
       "1     [talking, driver, 'm, goinghe, said, 'd, love,...   \n",
       "2     [anybody, know, rand, 's, likely, fall, dollar...   \n",
       "3               [miss, going, gigs, liverpool, unhappy]   \n",
       "4           [isnt, new, riverdale, tonight, ?, unhappy]   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, others, panel...   \n",
       "3870  [supreme, court, names, former, cag, head, 4-m...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, accus...   \n",
       "3872  [amulya, patnaik, appointed, new, delhi, polic...   \n",
       "\n",
       "                            misspealings+lemma+filtered  \\\n",
       "0                          [unhappy, dog, like, though]   \n",
       "1     [talking, driver, 'm, going, said, 'd, love, g...   \n",
       "2     [doe, anybody, know, rand, 's, likely, fall, d...   \n",
       "3                    [miss, going, gig, liver, unhappy]   \n",
       "4                 [new, riverdamp, tonight, ?, unhappy]   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869         [former, captain, dian, educe, panel, run]   \n",
       "3870  [supreme, court, name, former, cag, head, 4-me...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, ha, appointe, new, dele, police,...   \n",
       "\n",
       "                                              ohe_tweet  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                       ohe_tokenization  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                           ohe_stemming  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                          ohe_stemming+  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                      ohe_lemmatization  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                       ohe_misspealings  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                 ohe_misspealings+lemma  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                              ohe_filtered+tokenization  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                        ohe_misspealings+lemma+filtered  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                               wc_tweet  \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[3873 rows x 20 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_wc = CountVectorizer(ngram_range=(1, 3))\n",
    "X_wc = vectorizer_wc.fit_transform((df['tweet']).astype('str'))\n",
    "df['wc_tweet'] = list(X_wc.toarray())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>stemming</th>\n",
       "      <th>stemming+</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>misspealings</th>\n",
       "      <th>misspealings+lemma</th>\n",
       "      <th>filtered+tokenization</th>\n",
       "      <th>misspealings+lemma+filtered</th>\n",
       "      <th>...</th>\n",
       "      <th>ohe_misspealings+lemma+filtered</th>\n",
       "      <th>wc_tweet</th>\n",
       "      <th>wc_tokenization</th>\n",
       "      <th>wc_stemming</th>\n",
       "      <th>wc_stemming+</th>\n",
       "      <th>wc_lemmatization</th>\n",
       "      <th>wc_misspealings</th>\n",
       "      <th>wc_misspealings+lemma</th>\n",
       "      <th>wc_filtered+tokenization</th>\n",
       "      <th>wc_misspealings+lemma+filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>negative</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[unhappy, dogs, like, though]</td>\n",
       "      <td>[unhappy, dog, like, though]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, driver, 'm, goinghe, said, 'd, love,...</td>\n",
       "      <td>[talking, driver, 'm, going, said, 'd, love, g...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[anybody, know, rand, 's, likely, fall, dollar...</td>\n",
       "      <td>[doe, anybody, know, rand, 's, likely, fall, d...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liverpool, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[miss, going, gigs, liverpool, unhappy]</td>\n",
       "      <td>[miss, going, gig, liver, unhappy]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[isnt, new, riverdale, tonight, ?, unhappy]</td>\n",
       "      <td>[new, riverdamp, tonight, ?, unhappy]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>IDFC official Vikram Limaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>former captain Diana Edulji are others in pan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, diana, edulji, others, panel...</td>\n",
       "      <td>[former, captain, dian, educe, panel, run]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[supreme, court, names, former, cag, as, head,...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, as, head, ...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, names, former, cag, head, 4-m...</td>\n",
       "      <td>[supreme, court, name, former, cag, head, 4-me...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, as, a...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, a, ac...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, accus...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[amulya, patnaik, has, been, appointed, new, d...</td>\n",
       "      <td>[amulya, patnaik, has, been, appoint, new, del...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appoint, new, delh...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appointed, new, de...</td>\n",
       "      <td>[amula, paik, hash, been, appointe, new, dele,...</td>\n",
       "      <td>[amula, paik, ha, been, appointe, new, dele, p...</td>\n",
       "      <td>[amulya, patnaik, appointed, new, delhi, polic...</td>\n",
       "      <td>[amula, paik, ha, appointe, new, dele, police,...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet      tone  \\\n",
       "0                 How unhappy  some dogs like it though  negative   \n",
       "1     talking to my over driver about where I'm goin...  negative   \n",
       "2     Does anybody know if the Rand's likely to fall...  negative   \n",
       "3            I miss going to gigs in Liverpool unhappy   negative   \n",
       "4         There isnt a new Riverdale tonight ? unhappy   negative   \n",
       "...                                                 ...       ...   \n",
       "3868                        IDFC official Vikram Limaye   neutral   \n",
       "3869   former captain Diana Edulji are others in pan...   neutral   \n",
       "3870  Supreme Court names former CAG as head of 4-me...   neutral   \n",
       "3871  Court summons CM suspended BJP MP as accused i...   neutral   \n",
       "3872  Amulya Patnaik has been appointed new Delhi po...   neutral   \n",
       "\n",
       "                                           tokenization  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3     [i, miss, going, to, gigs, in, liverpool, unha...   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, names, former, cag, as, head,...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, as, a...   \n",
       "3872  [amulya, patnaik, has, been, appointed, new, d...   \n",
       "\n",
       "                                               stemming  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, has, been, appoint, new, del...   \n",
       "\n",
       "                                              stemming+  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, ha, been, appoint, new, delh...   \n",
       "\n",
       "                                          lemmatization  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3     [i, miss, going, to, gig, in, liverpool, unhappy]   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, a, ac...   \n",
       "3872  [amulya, patnaik, ha, been, appointed, new, de...   \n",
       "\n",
       "                                           misspealings  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, as, head, ...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, hash, been, appointe, new, dele,...   \n",
       "\n",
       "                                     misspealings+lemma  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, ha, been, appointe, new, dele, p...   \n",
       "\n",
       "                                  filtered+tokenization  \\\n",
       "0                         [unhappy, dogs, like, though]   \n",
       "1     [talking, driver, 'm, goinghe, said, 'd, love,...   \n",
       "2     [anybody, know, rand, 's, likely, fall, dollar...   \n",
       "3               [miss, going, gigs, liverpool, unhappy]   \n",
       "4           [isnt, new, riverdale, tonight, ?, unhappy]   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, others, panel...   \n",
       "3870  [supreme, court, names, former, cag, head, 4-m...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, accus...   \n",
       "3872  [amulya, patnaik, appointed, new, delhi, polic...   \n",
       "\n",
       "                            misspealings+lemma+filtered  ...  \\\n",
       "0                          [unhappy, dog, like, though]  ...   \n",
       "1     [talking, driver, 'm, going, said, 'd, love, g...  ...   \n",
       "2     [doe, anybody, know, rand, 's, likely, fall, d...  ...   \n",
       "3                    [miss, going, gig, liver, unhappy]  ...   \n",
       "4                 [new, riverdamp, tonight, ?, unhappy]  ...   \n",
       "...                                                 ...  ...   \n",
       "3868                     [id, official, viagram, liman]  ...   \n",
       "3869         [former, captain, dian, educe, panel, run]  ...   \n",
       "3870  [supreme, court, name, former, cag, head, 4-me...  ...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...  ...   \n",
       "3872  [amula, paik, ha, appointe, new, dele, police,...  ...   \n",
       "\n",
       "                        ohe_misspealings+lemma+filtered  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                               wc_tweet  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        wc_tokenization  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            wc_stemming  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           wc_stemming+  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                       wc_lemmatization  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        wc_misspealings  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                  wc_misspealings+lemma  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                               wc_filtered+tokenization  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                         wc_misspealings+lemma+filtered  \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[3873 rows x 28 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for way in preprocessing_ways:\n",
    "    X_wc = vectorizer_wc.fit_transform((df[way]).astype('str'))\n",
    "    df['wc_' + way] = list(X_wc.toarray())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1av-jRdzivW"
   },
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1qXdkQLzkbo",
    "outputId": "fc177c65-55bb-4c5b-97a5-87ef7b220e54"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>stemming</th>\n",
       "      <th>stemming+</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>misspealings</th>\n",
       "      <th>misspealings+lemma</th>\n",
       "      <th>filtered+tokenization</th>\n",
       "      <th>misspealings+lemma+filtered</th>\n",
       "      <th>...</th>\n",
       "      <th>wc_tweet</th>\n",
       "      <th>wc_tokenization</th>\n",
       "      <th>wc_stemming</th>\n",
       "      <th>wc_stemming+</th>\n",
       "      <th>wc_lemmatization</th>\n",
       "      <th>wc_misspealings</th>\n",
       "      <th>wc_misspealings+lemma</th>\n",
       "      <th>wc_filtered+tokenization</th>\n",
       "      <th>wc_misspealings+lemma+filtered</th>\n",
       "      <th>tfidf_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>negative</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[unhappy, dogs, like, though]</td>\n",
       "      <td>[unhappy, dog, like, though]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, driver, 'm, goinghe, said, 'd, love,...</td>\n",
       "      <td>[talking, driver, 'm, going, said, 'd, love, g...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[anybody, know, rand, 's, likely, fall, dollar...</td>\n",
       "      <td>[doe, anybody, know, rand, 's, likely, fall, d...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liverpool, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[miss, going, gigs, liverpool, unhappy]</td>\n",
       "      <td>[miss, going, gig, liver, unhappy]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[isnt, new, riverdale, tonight, ?, unhappy]</td>\n",
       "      <td>[new, riverdamp, tonight, ?, unhappy]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>IDFC official Vikram Limaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>former captain Diana Edulji are others in pan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, diana, edulji, others, panel...</td>\n",
       "      <td>[former, captain, dian, educe, panel, run]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[supreme, court, names, former, cag, as, head,...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, as, head, ...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, names, former, cag, head, 4-m...</td>\n",
       "      <td>[supreme, court, name, former, cag, head, 4-me...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, as, a...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, a, ac...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, accus...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[amulya, patnaik, has, been, appointed, new, d...</td>\n",
       "      <td>[amulya, patnaik, has, been, appoint, new, del...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appoint, new, delh...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appointed, new, de...</td>\n",
       "      <td>[amula, paik, hash, been, appointe, new, dele,...</td>\n",
       "      <td>[amula, paik, ha, been, appointe, new, dele, p...</td>\n",
       "      <td>[amulya, patnaik, appointed, new, delhi, polic...</td>\n",
       "      <td>[amula, paik, ha, appointe, new, dele, police,...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet      tone  \\\n",
       "0                 How unhappy  some dogs like it though  negative   \n",
       "1     talking to my over driver about where I'm goin...  negative   \n",
       "2     Does anybody know if the Rand's likely to fall...  negative   \n",
       "3            I miss going to gigs in Liverpool unhappy   negative   \n",
       "4         There isnt a new Riverdale tonight ? unhappy   negative   \n",
       "...                                                 ...       ...   \n",
       "3868                        IDFC official Vikram Limaye   neutral   \n",
       "3869   former captain Diana Edulji are others in pan...   neutral   \n",
       "3870  Supreme Court names former CAG as head of 4-me...   neutral   \n",
       "3871  Court summons CM suspended BJP MP as accused i...   neutral   \n",
       "3872  Amulya Patnaik has been appointed new Delhi po...   neutral   \n",
       "\n",
       "                                           tokenization  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3     [i, miss, going, to, gigs, in, liverpool, unha...   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, names, former, cag, as, head,...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, as, a...   \n",
       "3872  [amulya, patnaik, has, been, appointed, new, d...   \n",
       "\n",
       "                                               stemming  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, has, been, appoint, new, del...   \n",
       "\n",
       "                                              stemming+  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, ha, been, appoint, new, delh...   \n",
       "\n",
       "                                          lemmatization  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3     [i, miss, going, to, gig, in, liverpool, unhappy]   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, a, ac...   \n",
       "3872  [amulya, patnaik, ha, been, appointed, new, de...   \n",
       "\n",
       "                                           misspealings  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, as, head, ...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, hash, been, appointe, new, dele,...   \n",
       "\n",
       "                                     misspealings+lemma  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, ha, been, appointe, new, dele, p...   \n",
       "\n",
       "                                  filtered+tokenization  \\\n",
       "0                         [unhappy, dogs, like, though]   \n",
       "1     [talking, driver, 'm, goinghe, said, 'd, love,...   \n",
       "2     [anybody, know, rand, 's, likely, fall, dollar...   \n",
       "3               [miss, going, gigs, liverpool, unhappy]   \n",
       "4           [isnt, new, riverdale, tonight, ?, unhappy]   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, others, panel...   \n",
       "3870  [supreme, court, names, former, cag, head, 4-m...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, accus...   \n",
       "3872  [amulya, patnaik, appointed, new, delhi, polic...   \n",
       "\n",
       "                            misspealings+lemma+filtered  ...  \\\n",
       "0                          [unhappy, dog, like, though]  ...   \n",
       "1     [talking, driver, 'm, going, said, 'd, love, g...  ...   \n",
       "2     [doe, anybody, know, rand, 's, likely, fall, d...  ...   \n",
       "3                    [miss, going, gig, liver, unhappy]  ...   \n",
       "4                 [new, riverdamp, tonight, ?, unhappy]  ...   \n",
       "...                                                 ...  ...   \n",
       "3868                     [id, official, viagram, liman]  ...   \n",
       "3869         [former, captain, dian, educe, panel, run]  ...   \n",
       "3870  [supreme, court, name, former, cag, head, 4-me...  ...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...  ...   \n",
       "3872  [amula, paik, ha, appointe, new, dele, police,...  ...   \n",
       "\n",
       "                                               wc_tweet  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        wc_tokenization  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            wc_stemming  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           wc_stemming+  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                       wc_lemmatization  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        wc_misspealings  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                  wc_misspealings+lemma  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                               wc_filtered+tokenization  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                         wc_misspealings+lemma+filtered  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            tfidf_tweet  \n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                 ...  \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[3873 rows x 29 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform((df['tweet']).astype('str'))\n",
    "df['tfidf_tweet'] = list(X_tfidf.toarray())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "g32Fg3ra52zR"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tone</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>stemming</th>\n",
       "      <th>stemming+</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>misspealings</th>\n",
       "      <th>misspealings+lemma</th>\n",
       "      <th>filtered+tokenization</th>\n",
       "      <th>misspealings+lemma+filtered</th>\n",
       "      <th>...</th>\n",
       "      <th>wc_misspealings+lemma+filtered</th>\n",
       "      <th>tfidf_tweet</th>\n",
       "      <th>tfidf_tokenization</th>\n",
       "      <th>tfidf_stemming</th>\n",
       "      <th>tfidf_stemming+</th>\n",
       "      <th>tfidf_lemmatization</th>\n",
       "      <th>tfidf_misspealings</th>\n",
       "      <th>tfidf_misspealings+lemma</th>\n",
       "      <th>tfidf_filtered+tokenization</th>\n",
       "      <th>tfidf_misspealings+lemma+filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>negative</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[unhappy, dogs, like, though]</td>\n",
       "      <td>[unhappy, dog, like, though]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, driver, 'm, goinghe, said, 'd, love,...</td>\n",
       "      <td>[talking, driver, 'm, going, said, 'd, love, g...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[does, anybody, know, if, the, rand, 's, likel...</td>\n",
       "      <td>[doe, anybody, know, if, the, rand, 's, likely...</td>\n",
       "      <td>[anybody, know, rand, 's, likely, fall, dollar...</td>\n",
       "      <td>[doe, anybody, know, rand, 's, likely, fall, d...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, miss, going, to, gigs, in, liverpool, unha...</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liverpool, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[i, miss, going, to, gig, in, liver, unhappy]</td>\n",
       "      <td>[miss, going, gigs, liverpool, unhappy]</td>\n",
       "      <td>[miss, going, gig, liver, unhappy]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>negative</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[there, isnt, a, new, riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[there, is, a, new, riverdamp, tonight, ?, unh...</td>\n",
       "      <td>[isnt, new, riverdale, tonight, ?, unhappy]</td>\n",
       "      <td>[new, riverdamp, tonight, ?, unhappy]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>IDFC official Vikram Limaye</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, offici, vikram, limay]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>[idfc, official, vikram, limaye]</td>\n",
       "      <td>[id, official, viagram, liman]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>former captain Diana Edulji are others in pan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, other, i...</td>\n",
       "      <td>[former, captain, diana, edulji, are, others, ...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, dian, educe, are, other, in,...</td>\n",
       "      <td>[former, captain, diana, edulji, others, panel...</td>\n",
       "      <td>[former, captain, dian, educe, panel, run]</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[supreme, court, names, former, cag, as, head,...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, name, former, cag, as, head, ...</td>\n",
       "      <td>[supreme, court, name, former, cag, a, head, o...</td>\n",
       "      <td>[supreme, court, names, former, cag, head, 4-m...</td>\n",
       "      <td>[supreme, court, name, former, cag, head, 4-me...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, as, a...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, a, ac...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>[court, summons, cm, suspended, bjp, mp, accus...</td>\n",
       "      <td>[court, summons, cm, suspended, bobjerom, mp, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[amulya, patnaik, has, been, appointed, new, d...</td>\n",
       "      <td>[amulya, patnaik, has, been, appoint, new, del...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appoint, new, delh...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appointed, new, de...</td>\n",
       "      <td>[amula, paik, hash, been, appointe, new, dele,...</td>\n",
       "      <td>[amula, paik, ha, been, appointe, new, dele, p...</td>\n",
       "      <td>[amulya, patnaik, appointed, new, delhi, polic...</td>\n",
       "      <td>[amula, paik, ha, appointe, new, dele, police,...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3873 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet      tone  \\\n",
       "0                 How unhappy  some dogs like it though  negative   \n",
       "1     talking to my over driver about where I'm goin...  negative   \n",
       "2     Does anybody know if the Rand's likely to fall...  negative   \n",
       "3            I miss going to gigs in Liverpool unhappy   negative   \n",
       "4         There isnt a new Riverdale tonight ? unhappy   negative   \n",
       "...                                                 ...       ...   \n",
       "3868                        IDFC official Vikram Limaye   neutral   \n",
       "3869   former captain Diana Edulji are others in pan...   neutral   \n",
       "3870  Supreme Court names former CAG as head of 4-me...   neutral   \n",
       "3871  Court summons CM suspended BJP MP as accused i...   neutral   \n",
       "3872  Amulya Patnaik has been appointed new Delhi po...   neutral   \n",
       "\n",
       "                                           tokenization  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3     [i, miss, going, to, gigs, in, liverpool, unha...   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, names, former, cag, as, head,...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, as, a...   \n",
       "3872  [amulya, patnaik, has, been, appointed, new, d...   \n",
       "\n",
       "                                               stemming  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, has, been, appoint, new, del...   \n",
       "\n",
       "                                              stemming+  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "3868                      [idfc, offici, vikram, limay]   \n",
       "3869  [former, captain, diana, edulji, are, other, i...   \n",
       "3870  [suprem, court, name, former, cag, as, head, o...   \n",
       "3871  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "3872  [amulya, patnaik, ha, been, appoint, new, delh...   \n",
       "\n",
       "                                          lemmatization  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3     [i, miss, going, to, gig, in, liverpool, unhappy]   \n",
       "4     [there, isnt, a, new, riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, are, others, ...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, a, ac...   \n",
       "3872  [amulya, patnaik, ha, been, appointed, new, de...   \n",
       "\n",
       "                                           misspealings  \\\n",
       "0          [how, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [does, anybody, know, if, the, rand, 's, likel...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, as, head, ...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, hash, been, appointe, new, dele,...   \n",
       "\n",
       "                                     misspealings+lemma  \\\n",
       "0           [how, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [doe, anybody, know, if, the, rand, 's, likely...   \n",
       "3         [i, miss, going, to, gig, in, liver, unhappy]   \n",
       "4     [there, is, a, new, riverdamp, tonight, ?, unh...   \n",
       "...                                                 ...   \n",
       "3868                     [id, official, viagram, liman]   \n",
       "3869  [former, captain, dian, educe, are, other, in,...   \n",
       "3870  [supreme, court, name, former, cag, a, head, o...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...   \n",
       "3872  [amula, paik, ha, been, appointe, new, dele, p...   \n",
       "\n",
       "                                  filtered+tokenization  \\\n",
       "0                         [unhappy, dogs, like, though]   \n",
       "1     [talking, driver, 'm, goinghe, said, 'd, love,...   \n",
       "2     [anybody, know, rand, 's, likely, fall, dollar...   \n",
       "3               [miss, going, gigs, liverpool, unhappy]   \n",
       "4           [isnt, new, riverdale, tonight, ?, unhappy]   \n",
       "...                                                 ...   \n",
       "3868                   [idfc, official, vikram, limaye]   \n",
       "3869  [former, captain, diana, edulji, others, panel...   \n",
       "3870  [supreme, court, names, former, cag, head, 4-m...   \n",
       "3871  [court, summons, cm, suspended, bjp, mp, accus...   \n",
       "3872  [amulya, patnaik, appointed, new, delhi, polic...   \n",
       "\n",
       "                            misspealings+lemma+filtered  ...  \\\n",
       "0                          [unhappy, dog, like, though]  ...   \n",
       "1     [talking, driver, 'm, going, said, 'd, love, g...  ...   \n",
       "2     [doe, anybody, know, rand, 's, likely, fall, d...  ...   \n",
       "3                    [miss, going, gig, liver, unhappy]  ...   \n",
       "4                 [new, riverdamp, tonight, ?, unhappy]  ...   \n",
       "...                                                 ...  ...   \n",
       "3868                     [id, official, viagram, liman]  ...   \n",
       "3869         [former, captain, dian, educe, panel, run]  ...   \n",
       "3870  [supreme, court, name, former, cag, head, 4-me...  ...   \n",
       "3871  [court, summons, cm, suspended, bobjerom, mp, ...  ...   \n",
       "3872  [amula, paik, ha, appointe, new, dele, police,...  ...   \n",
       "\n",
       "                         wc_misspealings+lemma+filtered  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3869  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3870  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3871  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3872  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            tfidf_tweet  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                     tfidf_tokenization  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                         tfidf_stemming  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                        tfidf_stemming+  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                    tfidf_lemmatization  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                     tfidf_misspealings  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                               tfidf_misspealings+lemma  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                            tfidf_filtered+tokenization  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                      tfidf_misspealings+lemma+filtered  \n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                 ...  \n",
       "3868  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3869  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3870  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3871  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3872  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[3873 rows x 37 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for way in preprocessing_ways:\n",
    "    X_tfidf = vectorizer_tfidf.fit_transform((df[way]).astype('str'))\n",
    "    df['tfidf_' + way] = list(X_tfidf.toarray())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8fOQxVV524u"
   },
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_w2v = word2vec.Word2Vec(df['tokenization'], min_count=1, vector_size = 100, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('connect', 0.9615136981010437),\n",
       " ('recent', 0.804908037185669),\n",
       " ('thursday', 0.7111150622367859),\n",
       " ('follow', 0.45943012833595276),\n",
       " ('trick', 0.44421571493148804),\n",
       " ('commando', 0.32795336842536926),\n",
       " ('great', 0.3107462227344513),\n",
       " ('smooth', 0.3075921833515167),\n",
       " ('ought', 0.29486629366874695),\n",
       " ('thanks', 0.29220837354660034)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_w2v.wv.most_similar('happy', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comfortable', 0.3027094006538391),\n",
       " ('veggies', 0.3018379509449005),\n",
       " ('beginning', 0.2949639558792114),\n",
       " ('survey', 0.2780837416648865),\n",
       " ('bae', 0.2667633891105652),\n",
       " ('charlie', 0.24233095347881317),\n",
       " ('tabled', 0.24142755568027496),\n",
       " ('explode', 0.23658089339733124),\n",
       " ('shoutout', 0.23338963091373444),\n",
       " ('clauditte', 0.22291675209999084)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_w2v.wv.most_similar('positive', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('adblocker', 0.30785346031188965),\n",
       " ('insight', 0.3030245304107666),\n",
       " ('amanda', 0.3008870780467987),\n",
       " ('annette', 0.2957611680030823),\n",
       " ('lab', 0.27527397871017456),\n",
       " ('sunnyside', 0.24441593885421753),\n",
       " ('stealing', 0.24241776764392853),\n",
       " ('157', 0.24111859500408173),\n",
       " ('sketches', 0.2361643761396408),\n",
       " ('ios', 0.2355777621269226)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_w2v.wv.most_similar('unhappy', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Zg2-HvOAqLs"
   },
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top-10 most similar tweets for all the 18 ways of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ohe_tweet',\n",
       " 'ohe_tokenization',\n",
       " 'ohe_stemming',\n",
       " 'ohe_stemming+',\n",
       " 'ohe_lemmatization',\n",
       " 'ohe_misspealings',\n",
       " 'ohe_misspealings+lemma',\n",
       " 'ohe_filtered+tokenization',\n",
       " 'ohe_misspealings+lemma+filtered',\n",
       " 'wc_tweet',\n",
       " 'wc_tokenization',\n",
       " 'wc_stemming',\n",
       " 'wc_stemming+',\n",
       " 'wc_lemmatization',\n",
       " 'wc_misspealings',\n",
       " 'wc_misspealings+lemma',\n",
       " 'wc_filtered+tokenization',\n",
       " 'wc_misspealings+lemma+filtered',\n",
       " 'tfidf_tweet',\n",
       " 'tfidf_tokenization',\n",
       " 'tfidf_stemming',\n",
       " 'tfidf_stemming+',\n",
       " 'tfidf_lemmatization',\n",
       " 'tfidf_misspealings',\n",
       " 'tfidf_misspealings+lemma',\n",
       " 'tfidf_filtered+tokenization',\n",
       " 'tfidf_misspealings+lemma+filtered']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_columns = [column for column in df.columns if column.startswith((\"ohe\", \"wc\", \"tfidf\"))]\n",
    "vector_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_similar(array_coisine, n=10):\n",
    "    c = 0\n",
    "    list_indexes = []\n",
    "    index_repeate = []\n",
    "    limit = 0.9999999999999\n",
    "    while c < n:\n",
    "        ind = np.unravel_index(np.argmax(array_coisine, axis=None), array_coisine.shape)\n",
    "        ind1, ind2 = ind\n",
    "        if array_coisine[ind1][ind2] < limit and ind1 not in index_repeate and ind2 not in index_repeate and ind1 != ind2:\n",
    "            value = array_coisine[ind1][ind2]\n",
    "            list_indexes.append((ind, value))\n",
    "            index_repeate.extend([ind1, ind2])\n",
    "            c += 1\n",
    "        array_coisine[ind1][ind2] = -1\n",
    "    return list_indexes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "example np coisine array\n",
    "\n",
    "[[1.         0.03593269 0.11665286 ... 0.         0.         0.        ]\n",
    " [0.03593269 1.         0.08555322 ... 0.03024679 0.         0.02919552]\n",
    " [0.11665286 0.08555322 1.         ... 0.01713491 0.         0.        ]\n",
    " ...\n",
    " [0.         0.03024679 0.01713491 ... 1.         0.08514279 0.        ]\n",
    " [0.         0.         0.         ... 0.08514279 1.         0.        ]\n",
    " [0.         0.02919552 0.         ... 0.         0.         1.        ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tweets = df['tweet'].tolist()\n",
    "def print_similar(text_vectors):\n",
    "    array = np.array(df[text_vectors].tolist())\n",
    "    coisine_array = cosine_similarity(array)\n",
    "    print(\"coisine_array\\n\", coisine_array)\n",
    "    list_indexes = top_similar(coisine_array)\n",
    "    list_similar = [(list_tweets[ind1], list_tweets[ind2], score) for (ind1, ind2), score in list_indexes]\n",
    "    df_similar = pd.DataFrame(list_similar, columns =['phrase1', 'phrase2', 'score'])\n",
    "    print(f'text vector \"{text_vectors}\":\\n{json.dumps(df_similar.to_dict(\"records\"), indent=2)}\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coisine_array\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "text vector \"ohe_tweet\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"How unhappy  some dogs like it though\",\n",
      "    \"phrase2\": \"talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not\",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy \",\n",
      "    \"phrase2\": \"I miss going to gigs in Liverpool unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"There isnt a new Riverdale tonight ? unhappy \",\n",
      "    \"phrase2\": \"it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu\",\n",
      "    \"phrase2\": \"don't like how jittery caffeine makes me sad \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"My area's not on the list unhappy  think I'll go LibDems anyway\",\n",
      "    \"phrase2\": \"I want fun plans this weekend unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"When can you notice me.  unhappy  what?  \",\n",
      "    \"phrase2\": \"Ahhhhh! You recognized LOGAN!!! Cinemax shows have a BAD track record for getting cancelled unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Errr dude.... They're gone unhappy  Asked other league memeber to check  the guys are go \",\n",
      "    \"phrase2\": \"Not you again sad  \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Why would Harvey be going to prison? unhappy \",\n",
      "    \"phrase2\": \"Missing in crying  Seaside area. \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Becoz if we will depend on your promoting its waste of hardwork to all team who \",\n",
      "    \"phrase2\": \"I thought you'll save me crying \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"major waffle cravings right now sad \",\n",
      "    \"phrase2\": \"cant speak japanese ::(\",\n",
      "    \"score\": 0.0\n",
      "  }\n",
      "]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text_vectors in vector_columns[:1]:\n",
    "    print_similar(text_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coisine_array\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "text vector \"ohe_tokenization\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"How unhappy  some dogs like it though\",\n",
      "    \"phrase2\": \"talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not\",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy \",\n",
      "    \"phrase2\": \"I miss going to gigs in Liverpool unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"There isnt a new Riverdale tonight ? unhappy \",\n",
      "    \"phrase2\": \"it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu\",\n",
      "    \"phrase2\": \"don't like how jittery caffeine makes me sad \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"My area's not on the list unhappy  think I'll go LibDems anyway\",\n",
      "    \"phrase2\": \"I want fun plans this weekend unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"When can you notice me.  unhappy  what?  \",\n",
      "    \"phrase2\": \"Ahhhhh! You recognized LOGAN!!! Cinemax shows have a BAD track record for getting cancelled unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Errr dude.... They're gone unhappy  Asked other league memeber to check  the guys are go \",\n",
      "    \"phrase2\": \"Not you again sad  \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Why would Harvey be going to prison? unhappy \",\n",
      "    \"phrase2\": \"Missing in crying  Seaside area. \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Becoz if we will depend on your promoting its waste of hardwork to all team who \",\n",
      "    \"phrase2\": \"I thought you'll save me crying \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"major waffle cravings right now sad \",\n",
      "    \"phrase2\": \"cant speak japanese ::(\",\n",
      "    \"score\": 0.0\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "text vector \"ohe_stemming\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"How unhappy  some dogs like it though\",\n",
      "    \"phrase2\": \"talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not\",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy \",\n",
      "    \"phrase2\": \"I miss going to gigs in Liverpool unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"There isnt a new Riverdale tonight ? unhappy \",\n",
      "    \"phrase2\": \"it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu\",\n",
      "    \"phrase2\": \"don't like how jittery caffeine makes me sad \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"My area's not on the list unhappy  think I'll go LibDems anyway\",\n",
      "    \"phrase2\": \"I want fun plans this weekend unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"When can you notice me.  unhappy  what?  \",\n",
      "    \"phrase2\": \"Ahhhhh! You recognized LOGAN!!! Cinemax shows have a BAD track record for getting cancelled unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Errr dude.... They're gone unhappy  Asked other league memeber to check  the guys are go \",\n",
      "    \"phrase2\": \"Not you again sad  \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Why would Harvey be going to prison? unhappy \",\n",
      "    \"phrase2\": \"Missing in crying  Seaside area. \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Becoz if we will depend on your promoting its waste of hardwork to all team who \",\n",
      "    \"phrase2\": \"I thought you'll save me crying \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"major waffle cravings right now sad \",\n",
      "    \"phrase2\": \"cant speak japanese ::(\",\n",
      "    \"score\": 0.0\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "text vector \"ohe_stemming+\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"How unhappy  some dogs like it though\",\n",
      "    \"phrase2\": \"talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not\",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy \",\n",
      "    \"phrase2\": \"I miss going to gigs in Liverpool unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"There isnt a new Riverdale tonight ? unhappy \",\n",
      "    \"phrase2\": \"it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu\",\n",
      "    \"phrase2\": \"don't like how jittery caffeine makes me sad \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"My area's not on the list unhappy  think I'll go LibDems anyway\",\n",
      "    \"phrase2\": \"I want fun plans this weekend unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"When can you notice me.  unhappy  what?  \",\n",
      "    \"phrase2\": \"Ahhhhh! You recognized LOGAN!!! Cinemax shows have a BAD track record for getting cancelled unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Errr dude.... They're gone unhappy  Asked other league memeber to check  the guys are go \",\n",
      "    \"phrase2\": \"Not you again sad  \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Why would Harvey be going to prison? unhappy \",\n",
      "    \"phrase2\": \"Missing in crying  Seaside area. \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Becoz if we will depend on your promoting its waste of hardwork to all team who \",\n",
      "    \"phrase2\": \"I thought you'll save me crying \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"major waffle cravings right now sad \",\n",
      "    \"phrase2\": \"cant speak japanese ::(\",\n",
      "    \"score\": 0.0\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "text vector \"ohe_lemmatization\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"How unhappy  some dogs like it though\",\n",
      "    \"phrase2\": \"talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not\",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy \",\n",
      "    \"phrase2\": \"I miss going to gigs in Liverpool unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"There isnt a new Riverdale tonight ? unhappy \",\n",
      "    \"phrase2\": \"it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu\",\n",
      "    \"phrase2\": \"don't like how jittery caffeine makes me sad \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"My area's not on the list unhappy  think I'll go LibDems anyway\",\n",
      "    \"phrase2\": \"I want fun plans this weekend unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"When can you notice me.  unhappy  what?  \",\n",
      "    \"phrase2\": \"Ahhhhh! You recognized LOGAN!!! Cinemax shows have a BAD track record for getting cancelled unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Errr dude.... They're gone unhappy  Asked other league memeber to check  the guys are go \",\n",
      "    \"phrase2\": \"Not you again sad  \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Why would Harvey be going to prison? unhappy \",\n",
      "    \"phrase2\": \"Missing in crying  Seaside area. \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Becoz if we will depend on your promoting its waste of hardwork to all team who \",\n",
      "    \"phrase2\": \"I thought you'll save me crying \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"major waffle cravings right now sad \",\n",
      "    \"phrase2\": \"cant speak japanese ::(\",\n",
      "    \"score\": 0.0\n",
      "  }\n",
      "]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coisine_array\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "text vector \"ohe_misspealings\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"How unhappy  some dogs like it though\",\n",
      "    \"phrase2\": \"talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not\",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy \",\n",
      "    \"phrase2\": \"I miss going to gigs in Liverpool unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"There isnt a new Riverdale tonight ? unhappy \",\n",
      "    \"phrase2\": \"it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu\",\n",
      "    \"phrase2\": \"don't like how jittery caffeine makes me sad \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"My area's not on the list unhappy  think I'll go LibDems anyway\",\n",
      "    \"phrase2\": \"I want fun plans this weekend unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"When can you notice me.  unhappy  what?  \",\n",
      "    \"phrase2\": \"Ahhhhh! You recognized LOGAN!!! Cinemax shows have a BAD track record for getting cancelled unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Errr dude.... They're gone unhappy  Asked other league memeber to check  the guys are go \",\n",
      "    \"phrase2\": \"Not you again sad  \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Why would Harvey be going to prison? unhappy \",\n",
      "    \"phrase2\": \"Missing in crying  Seaside area. \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Becoz if we will depend on your promoting its waste of hardwork to all team who \",\n",
      "    \"phrase2\": \"I thought you'll save me crying \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"major waffle cravings right now sad \",\n",
      "    \"phrase2\": \"cant speak japanese ::(\",\n",
      "    \"score\": 0.0\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "text vector \"ohe_misspealings+lemma\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"How unhappy  some dogs like it though\",\n",
      "    \"phrase2\": \"talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not\",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy \",\n",
      "    \"phrase2\": \"I miss going to gigs in Liverpool unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"There isnt a new Riverdale tonight ? unhappy \",\n",
      "    \"phrase2\": \"it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu\",\n",
      "    \"phrase2\": \"don't like how jittery caffeine makes me sad \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"My area's not on the list unhappy  think I'll go LibDems anyway\",\n",
      "    \"phrase2\": \"I want fun plans this weekend unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"When can you notice me.  unhappy  what?  \",\n",
      "    \"phrase2\": \"Ahhhhh! You recognized LOGAN!!! Cinemax shows have a BAD track record for getting cancelled unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Errr dude.... They're gone unhappy  Asked other league memeber to check  the guys are go \",\n",
      "    \"phrase2\": \"Not you again sad  \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Why would Harvey be going to prison? unhappy \",\n",
      "    \"phrase2\": \"Missing in crying  Seaside area. \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Becoz if we will depend on your promoting its waste of hardwork to all team who \",\n",
      "    \"phrase2\": \"I thought you'll save me crying \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"major waffle cravings right now sad \",\n",
      "    \"phrase2\": \"cant speak japanese ::(\",\n",
      "    \"score\": 0.0\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "text vector \"ohe_filtered+tokenization\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"How unhappy  some dogs like it though\",\n",
      "    \"phrase2\": \"talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not\",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy \",\n",
      "    \"phrase2\": \"I miss going to gigs in Liverpool unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"There isnt a new Riverdale tonight ? unhappy \",\n",
      "    \"phrase2\": \"it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu\",\n",
      "    \"phrase2\": \"don't like how jittery caffeine makes me sad \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"My area's not on the list unhappy  think I'll go LibDems anyway\",\n",
      "    \"phrase2\": \"I want fun plans this weekend unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"When can you notice me.  unhappy  what?  \",\n",
      "    \"phrase2\": \"Ahhhhh! You recognized LOGAN!!! Cinemax shows have a BAD track record for getting cancelled unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Errr dude.... They're gone unhappy  Asked other league memeber to check  the guys are go \",\n",
      "    \"phrase2\": \"Not you again sad  \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Why would Harvey be going to prison? unhappy \",\n",
      "    \"phrase2\": \"Missing in crying  Seaside area. \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Becoz if we will depend on your promoting its waste of hardwork to all team who \",\n",
      "    \"phrase2\": \"I thought you'll save me crying \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"major waffle cravings right now sad \",\n",
      "    \"phrase2\": \"cant speak japanese ::(\",\n",
      "    \"score\": 0.0\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "text vector \"ohe_misspealings+lemma+filtered\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"How unhappy  some dogs like it though\",\n",
      "    \"phrase2\": \"talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not\",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy \",\n",
      "    \"phrase2\": \"I miss going to gigs in Liverpool unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"There isnt a new Riverdale tonight ? unhappy \",\n",
      "    \"phrase2\": \"it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu\",\n",
      "    \"phrase2\": \"don't like how jittery caffeine makes me sad \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"My area's not on the list unhappy  think I'll go LibDems anyway\",\n",
      "    \"phrase2\": \"I want fun plans this weekend unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"When can you notice me.  unhappy  what?  \",\n",
      "    \"phrase2\": \"Ahhhhh! You recognized LOGAN!!! Cinemax shows have a BAD track record for getting cancelled unhappy \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Errr dude.... They're gone unhappy  Asked other league memeber to check  the guys are go \",\n",
      "    \"phrase2\": \"Not you again sad  \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Why would Harvey be going to prison? unhappy \",\n",
      "    \"phrase2\": \"Missing in crying  Seaside area. \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Becoz if we will depend on your promoting its waste of hardwork to all team who \",\n",
      "    \"phrase2\": \"I thought you'll save me crying \",\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"major waffle cravings right now sad \",\n",
      "    \"phrase2\": \"cant speak japanese ::(\",\n",
      "    \"score\": 0.0\n",
      "  }\n",
      "]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coisine_array\n",
      " [[1.         0.02777778 0.08111071 ... 0.         0.         0.        ]\n",
      " [0.02777778 1.         0.10814761 ... 0.05521576 0.         0.01719035]\n",
      " [0.08111071 0.10814761 1.         ... 0.03582872 0.         0.        ]\n",
      " ...\n",
      " [0.         0.05521576 0.03582872 ... 1.         0.05205792 0.        ]\n",
      " [0.         0.         0.         ... 0.05205792 1.         0.        ]\n",
      " [0.         0.01719035 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"wc_tweet\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Koalas are dying of thirst  and it's all because of us unhappy  \",\n",
      "    \"phrase2\": \"are dying of thirst  and it's all because of us unhappy   \",\n",
      "    \"score\": 0.9561828874675149\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Koalas are dying of thirst  and it's all because of us unhappy  \",\n",
      "    \"phrase2\": \"are dying of thirst  and it's all because of us unhappy   \",\n",
      "    \"score\": 0.9561828874675149\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for.. cont1\",\n",
      "    \"score\": 0.9476095192139946\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"for the recent follow. Much appreciated happy  Want this?\",\n",
      "    \"phrase2\": \"thanks for the recent follow. Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9428090415820635\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? It's FREE!\",\n",
      "    \"score\": 0.9339916624531052\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1.         0.02777778 0.08111071 ... 0.         0.         0.        ]\n",
      " [0.02777778 1.         0.10814761 ... 0.05521576 0.         0.01719035]\n",
      " [0.08111071 0.10814761 1.         ... 0.03582872 0.         0.        ]\n",
      " ...\n",
      " [0.         0.05521576 0.03582872 ... 1.         0.05205792 0.        ]\n",
      " [0.         0.         0.         ... 0.05205792 1.         0.        ]\n",
      " [0.         0.01719035 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"wc_tokenization\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Koalas are dying of thirst  and it's all because of us unhappy  \",\n",
      "    \"phrase2\": \"are dying of thirst  and it's all because of us unhappy   \",\n",
      "    \"score\": 0.9561828874675149\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Koalas are dying of thirst  and it's all because of us unhappy  \",\n",
      "    \"phrase2\": \"are dying of thirst  and it's all because of us unhappy   \",\n",
      "    \"score\": 0.9561828874675149\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for.. cont1\",\n",
      "    \"score\": 0.9476095192139946\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"for the recent follow. Much appreciated happy  Want this?\",\n",
      "    \"phrase2\": \"thanks for the recent follow. Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9428090415820636\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? It's FREE!\",\n",
      "    \"score\": 0.9339916624531052\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1.         0.02777778 0.10814761 ... 0.         0.         0.        ]\n",
      " [0.02777778 1.         0.10814761 ... 0.05521576 0.         0.01719035]\n",
      " [0.10814761 0.10814761 1.         ... 0.03582872 0.         0.        ]\n",
      " ...\n",
      " [0.         0.05521576 0.03582872 ... 1.         0.05205792 0.        ]\n",
      " [0.         0.         0.         ... 0.05205792 1.         0.        ]\n",
      " [0.         0.01719035 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"wc_stemming\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Koalas are dying of thirst  and it's all because of us unhappy  \",\n",
      "    \"phrase2\": \"are dying of thirst  and it's all because of us unhappy   \",\n",
      "    \"score\": 0.956182887467515\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Koalas are dying of thirst  and it's all because of us unhappy  \",\n",
      "    \"phrase2\": \"are dying of thirst  and it's all because of us unhappy   \",\n",
      "    \"score\": 0.956182887467515\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for.. cont1\",\n",
      "    \"score\": 0.9476095192139946\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"for the recent follow. Much appreciated happy  Want this?\",\n",
      "    \"phrase2\": \"thanks for the recent follow. Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9428090415820636\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? It's FREE!\",\n",
      "    \"score\": 0.9339916624531052\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  }\n",
      "]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coisine_array\n",
      " [[1.         0.02777778 0.10814761 ... 0.         0.         0.        ]\n",
      " [0.02777778 1.         0.10814761 ... 0.05521576 0.         0.01719035]\n",
      " [0.10814761 0.10814761 1.         ... 0.03582872 0.         0.        ]\n",
      " ...\n",
      " [0.         0.05521576 0.03582872 ... 1.         0.05205792 0.        ]\n",
      " [0.         0.         0.         ... 0.05205792 1.         0.        ]\n",
      " [0.         0.01719035 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"wc_stemming+\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Koalas are dying of thirst  and it's all because of us unhappy  \",\n",
      "    \"phrase2\": \"are dying of thirst  and it's all because of us unhappy   \",\n",
      "    \"score\": 0.9561828874675151\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Koalas are dying of thirst  and it's all because of us unhappy  \",\n",
      "    \"phrase2\": \"are dying of thirst  and it's all because of us unhappy   \",\n",
      "    \"score\": 0.9561828874675151\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for.. cont1\",\n",
      "    \"score\": 0.9476095192139946\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"for the recent follow. Much appreciated happy  Want this?\",\n",
      "    \"phrase2\": \"thanks for the recent follow. Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9428090415820636\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? It's FREE!\",\n",
      "    \"score\": 0.9339916624531052\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1.         0.02777778 0.08111071 ... 0.         0.         0.        ]\n",
      " [0.02777778 1.         0.10814761 ... 0.05735393 0.         0.01719035]\n",
      " [0.08111071 0.10814761 1.         ... 0.03721615 0.         0.        ]\n",
      " ...\n",
      " [0.         0.05735393 0.03721615 ... 1.         0.02823912 0.        ]\n",
      " [0.         0.         0.         ... 0.02823912 1.         0.        ]\n",
      " [0.         0.01719035 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"wc_lemmatization\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Koalas are dying of thirst  and it's all because of us unhappy  \",\n",
      "    \"phrase2\": \"are dying of thirst  and it's all because of us unhappy   \",\n",
      "    \"score\": 0.9519716382329884\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Koalas are dying of thirst  and it's all because of us unhappy  \",\n",
      "    \"phrase2\": \"are dying of thirst  and it's all because of us unhappy   \",\n",
      "    \"score\": 0.9519716382329884\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for us to.. cont1\",\n",
      "    \"score\": 0.95095270097113\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"for the recent follow. Much appreciated happy  Want this?\",\n",
      "    \"phrase2\": \"thanks for the recent follow. Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9428090415820636\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? It's FREE!\",\n",
      "    \"score\": 0.9339916624531052\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1.         0.02777778 0.08111071 ... 0.         0.         0.        ]\n",
      " [0.02777778 1.         0.10814761 ... 0.05521576 0.         0.01719035]\n",
      " [0.08111071 0.10814761 1.         ... 0.03582872 0.         0.        ]\n",
      " ...\n",
      " [0.         0.05521576 0.03582872 ... 1.         0.05205792 0.        ]\n",
      " [0.         0.         0.         ... 0.05205792 1.         0.        ]\n",
      " [0.         0.01719035 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"wc_misspealings\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Koalas are dying of thirst  and it's all because of us unhappy  \",\n",
      "    \"phrase2\": \"are dying of thirst  and it's all because of us unhappy   \",\n",
      "    \"score\": 0.956182887467515\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Koalas are dying of thirst  and it's all because of us unhappy  \",\n",
      "    \"phrase2\": \"are dying of thirst  and it's all because of us unhappy   \",\n",
      "    \"score\": 0.956182887467515\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for.. cont1\",\n",
      "    \"score\": 0.9476095192139946\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"for the recent follow. Much appreciated happy  Want this?\",\n",
      "    \"phrase2\": \"thanks for the recent follow. Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9428090415820636\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? It's FREE!\",\n",
      "    \"score\": 0.9339916624531052\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  }\n",
      "]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coisine_array\n",
      " [[1.         0.02777778 0.08111071 ... 0.         0.         0.        ]\n",
      " [0.02777778 1.         0.10814761 ... 0.05735393 0.         0.01719035]\n",
      " [0.08111071 0.10814761 1.         ... 0.03721615 0.         0.        ]\n",
      " ...\n",
      " [0.         0.05735393 0.03721615 ... 1.         0.02823912 0.        ]\n",
      " [0.         0.         0.         ... 0.02823912 1.         0.        ]\n",
      " [0.         0.01719035 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"wc_misspealings+lemma\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Koalas are dying of thirst  and it's all because of us unhappy  \",\n",
      "    \"phrase2\": \"are dying of thirst  and it's all because of us unhappy   \",\n",
      "    \"score\": 0.9519716382329884\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Koalas are dying of thirst  and it's all because of us unhappy  \",\n",
      "    \"phrase2\": \"are dying of thirst  and it's all because of us unhappy   \",\n",
      "    \"score\": 0.9519716382329884\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for us to.. cont1\",\n",
      "    \"score\": 0.95095270097113\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"for the recent follow. Much appreciated happy  Want this?\",\n",
      "    \"phrase2\": \"thanks for the recent follow. Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9428090415820636\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? It's FREE!\",\n",
      "    \"score\": 0.9339916624531052\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.  Want this\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want it\",\n",
      "    \"score\": 0.9268292682926833\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1.         0.         0.05337605 ... 0.         0.         0.        ]\n",
      " [0.         1.         0.         ... 0.         0.         0.02961744]\n",
      " [0.05337605 0.         1.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 1.         0.03703704 0.        ]\n",
      " [0.         0.         0.         ... 0.03703704 1.         0.        ]\n",
      " [0.         0.02961744 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"wc_filtered+tokenization\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? It's FREE!\",\n",
      "    \"score\": 0.9468641529479987\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Get it\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.  Get FREE?\",\n",
      "    \"score\": 0.9468641529479987\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Get it\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.  Get FREE?\",\n",
      "    \"score\": 0.9468641529479987\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy  Want this ?\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"score\": 0.9428090415820635\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being my top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9428090415820635\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9428090415820635\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9428090415820635\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"score\": 0.9405399431259603\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"score\": 0.9405399431259603\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"score\": 0.9405399431259603\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1.         0.         0.05143445 ... 0.         0.         0.        ]\n",
      " [0.         1.         0.         ... 0.         0.         0.0285133 ]\n",
      " [0.05143445 0.         1.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 1.         0.03703704 0.        ]\n",
      " [0.         0.         0.         ... 0.03703704 1.         0.        ]\n",
      " [0.         0.0285133  0.         ... 0.         0.         1.        ]]\n",
      "text vector \"wc_misspealings+lemma+filtered\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this? It's FREE!\",\n",
      "    \"score\": 0.9468641529479989\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Get it\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.  Get FREE?\",\n",
      "    \"score\": 0.9468641529479987\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Get it\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.  Get FREE?\",\n",
      "    \"score\": 0.9468641529479987\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy  Want this ?\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"score\": 0.9428090415820635\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being my top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9428090415820635\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9428090415820635\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9428090415820635\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"score\": 0.9405399431259603\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"score\": 0.9405399431259603\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"score\": 0.9405399431259603\n",
      "  }\n",
      "]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coisine_array\n",
      " [[1.         0.03591963 0.11626938 ... 0.         0.         0.        ]\n",
      " [0.03591963 1.         0.08524099 ... 0.0302358  0.         0.02916743]\n",
      " [0.11626938 0.08524099 1.         ... 0.01707858 0.         0.        ]\n",
      " ...\n",
      " [0.         0.0302358  0.01707858 ... 1.         0.08514279 0.        ]\n",
      " [0.         0.         0.         ... 0.08514279 1.         0.        ]\n",
      " [0.         0.02916743 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"tfidf_tweet\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for.. cont1\",\n",
      "    \"score\": 0.9950209290254434\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy  Want this ?\",\n",
      "    \"phrase2\": \"Hey thanks for being my top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9781556912351734\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for us to.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number.. cont1\",\n",
      "    \"score\": 0.9763945896181662\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \" Tamil Nadu\",\n",
      "    \"phrase2\": \"In Tamil Nadu\",\n",
      "    \"score\": 0.9627733175258397\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy  Want this?\",\n",
      "    \"phrase2\": \"Share the love: thanks for being top new followers this week happy  Want it\",\n",
      "    \"score\": 0.9569699861550262\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.955202723515892\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.955202723515892\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \" and more. Also in epaper. \",\n",
      "    \"phrase2\": \" and more. Also in the epaper. \",\n",
      "    \"score\": 0.9548956731150144\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy\",\n",
      "    \"phrase2\": \"Share the love:thanks for being top new followers this week happy   Want this\",\n",
      "    \"score\": 0.9520127040137568\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"good morning unhappy \",\n",
      "    \"phrase2\": \"good morning\",\n",
      "    \"score\": 0.9505586142016142\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1.         0.03593269 0.11665286 ... 0.         0.         0.        ]\n",
      " [0.03593269 1.         0.08555322 ... 0.03024679 0.         0.02919552]\n",
      " [0.11665286 0.08555322 1.         ... 0.01713491 0.         0.        ]\n",
      " ...\n",
      " [0.         0.03024679 0.01713491 ... 1.         0.08514279 0.        ]\n",
      " [0.         0.         0.         ... 0.08514279 1.         0.        ]\n",
      " [0.         0.02919552 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"tfidf_tokenization\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for.. cont1\",\n",
      "    \"score\": 0.9950195868889363\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy  Want this ?\",\n",
      "    \"phrase2\": \"Hey thanks for being my top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9781556912351732\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for us to.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number.. cont1\",\n",
      "    \"score\": 0.9763885053408303\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \" Tamil Nadu\",\n",
      "    \"phrase2\": \"In Tamil Nadu\",\n",
      "    \"score\": 0.9627733175258397\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy  Want this?\",\n",
      "    \"phrase2\": \"Share the love: thanks for being top new followers this week happy  Want it\",\n",
      "    \"score\": 0.9569699861550264\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.955202723515892\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.955202723515892\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \" and more. Also in epaper. \",\n",
      "    \"phrase2\": \" and more. Also in the epaper. \",\n",
      "    \"score\": 0.9548956731150144\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy\",\n",
      "    \"phrase2\": \"Share the love:thanks for being top new followers this week happy   Want this\",\n",
      "    \"score\": 0.9520127040137568\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"good morning unhappy \",\n",
      "    \"phrase2\": \"good morning\",\n",
      "    \"score\": 0.9505586142016142\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1.         0.03567319 0.18016489 ... 0.         0.         0.        ]\n",
      " [0.03567319 1.         0.09084096 ... 0.03246321 0.         0.03005544]\n",
      " [0.18016489 0.09084096 1.         ... 0.01915491 0.         0.        ]\n",
      " ...\n",
      " [0.         0.03246321 0.01915491 ... 1.         0.0907867  0.        ]\n",
      " [0.         0.         0.         ... 0.0907867  1.         0.        ]\n",
      " [0.         0.03005544 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"tfidf_stemming\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for.. cont1\",\n",
      "    \"score\": 0.9946152771636159\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Stats for the day have arrived. 1 new follower and NO unfollowers happy  via\",\n",
      "    \"phrase2\": \"Stats for the day have arrived. to  new followers and NO unfollowers happy  via\",\n",
      "    \"score\": 0.9901945941142508\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy  Want this ?\",\n",
      "    \"phrase2\": \"Hey thanks for being my top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9755583335868863\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for us to.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number.. cont1\",\n",
      "    \"score\": 0.9746592172562856\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \" Tamil Nadu\",\n",
      "    \"phrase2\": \"In Tamil Nadu\",\n",
      "    \"score\": 0.962814554723202\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \" and more. Also in epaper. \",\n",
      "    \"phrase2\": \" and more. Also in the epaper. \",\n",
      "    \"score\": 0.9548907318980581\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"for the recent follow. Much appreciated happy  Want this?\",\n",
      "    \"phrase2\": \"thanks for the recent follow. Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9518336134506635\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy  Want this?\",\n",
      "    \"phrase2\": \"Share the love: thanks for being top new followers this week happy  Want it\",\n",
      "    \"score\": 0.9517867952002512\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9512735335017145\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9512735335017145\n",
      "  }\n",
      "]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coisine_array\n",
      " [[1.         0.03567319 0.18016489 ... 0.         0.         0.        ]\n",
      " [0.03567319 1.         0.09084096 ... 0.03239778 0.         0.03006588]\n",
      " [0.18016489 0.09084096 1.         ... 0.0191163  0.         0.        ]\n",
      " ...\n",
      " [0.         0.03239778 0.0191163  ... 1.         0.09082783 0.        ]\n",
      " [0.         0.         0.         ... 0.09082783 1.         0.        ]\n",
      " [0.         0.03006588 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"tfidf_stemming+\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for.. cont1\",\n",
      "    \"score\": 0.9945573313038333\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Stats for the day have arrived. 1 new follower and NO unfollowers happy  via\",\n",
      "    \"phrase2\": \"Stats for the day have arrived. to  new followers and NO unfollowers happy  via\",\n",
      "    \"score\": 0.9901945941142508\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy  Want this ?\",\n",
      "    \"phrase2\": \"Hey thanks for being my top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9755574390547335\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for us to.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number.. cont1\",\n",
      "    \"score\": 0.9743994804238301\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \" Tamil Nadu\",\n",
      "    \"phrase2\": \"In Tamil Nadu\",\n",
      "    \"score\": 0.962814554723202\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \" and more. Also in epaper. \",\n",
      "    \"phrase2\": \" and more. Also in the epaper. \",\n",
      "    \"score\": 0.9548907318980581\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy  Want this?\",\n",
      "    \"phrase2\": \"Share the love: thanks for being top new followers this week happy  Want it\",\n",
      "    \"score\": 0.9518356144392575\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"for the recent follow. Much appreciated happy  Want this?\",\n",
      "    \"phrase2\": \"thanks for the recent follow. Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9518252336503689\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9513129975421716\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9513129975421716\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1.         0.03512534 0.12140322 ... 0.         0.         0.        ]\n",
      " [0.03512534 1.         0.08673962 ... 0.03214847 0.         0.02945959]\n",
      " [0.12140322 0.08673962 1.         ... 0.01856256 0.         0.        ]\n",
      " ...\n",
      " [0.         0.03214847 0.01856256 ... 1.         0.05051432 0.        ]\n",
      " [0.         0.         0.         ... 0.05051432 1.         0.        ]\n",
      " [0.         0.02945959 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"tfidf_lemmatization\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for us to.. cont1\",\n",
      "    \"score\": 0.996148572341348\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Stats for the day have arrived. 1 new follower and NO unfollowers happy  via\",\n",
      "    \"phrase2\": \"Stats for the day have arrived. to  new followers and NO unfollowers happy  via\",\n",
      "    \"score\": 0.9909438866369251\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy  Want this ?\",\n",
      "    \"phrase2\": \"Hey thanks for being my top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9778634250512958\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \" Tamil Nadu\",\n",
      "    \"phrase2\": \"In Tamil Nadu\",\n",
      "    \"score\": 0.962814554723202\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy  Want this?\",\n",
      "    \"phrase2\": \"Share the love: thanks for being top new followers this week happy  Want it\",\n",
      "    \"score\": 0.9575315804888258\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9556595679730261\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9556595679730261\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \" and more. Also in epaper. \",\n",
      "    \"phrase2\": \" and more. Also in the epaper. \",\n",
      "    \"score\": 0.9548907318980581\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy\",\n",
      "    \"phrase2\": \"Share the love:thanks for being top new followers this week happy   Want this\",\n",
      "    \"score\": 0.9524139394544473\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"good morning unhappy \",\n",
      "    \"phrase2\": \"good morning\",\n",
      "    \"score\": 0.9505586142016142\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1.         0.03672354 0.12083935 ... 0.         0.         0.        ]\n",
      " [0.03672354 1.         0.08998993 ... 0.03208008 0.         0.03045744]\n",
      " [0.12083935 0.08998993 1.         ... 0.01842282 0.         0.        ]\n",
      " ...\n",
      " [0.         0.03208008 0.01842282 ... 1.         0.08881417 0.        ]\n",
      " [0.         0.         0.         ... 0.08881417 1.         0.        ]\n",
      " [0.         0.03045744 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"tfidf_misspealings\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for.. cont1\",\n",
      "    \"score\": 0.994772540727404\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Stats for the day have arrived. 1 new follower and NO unfollowers happy  via\",\n",
      "    \"phrase2\": \"Stats for the day have arrived. to  new followers and NO unfollowers happy  via\",\n",
      "    \"score\": 0.9908364214096361\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy  Want this ?\",\n",
      "    \"phrase2\": \"Hey thanks for being my top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9777033521982806\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for us to.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number.. cont1\",\n",
      "    \"score\": 0.9753654617889568\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \" Tamil Nadu\",\n",
      "    \"phrase2\": \"In Tamil Nadu\",\n",
      "    \"score\": 0.9621140036966208\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy  Want this?\",\n",
      "    \"phrase2\": \"Share the love: thanks for being top new followers this week happy  Want it\",\n",
      "    \"score\": 0.956132937716624\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9554571512262521\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9554571512262521\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \" and more. Also in epaper. \",\n",
      "    \"phrase2\": \" and more. Also in the epaper. \",\n",
      "    \"score\": 0.9548765852240565\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy\",\n",
      "    \"phrase2\": \"Share the love:thanks for being top new followers this week happy   Want this\",\n",
      "    \"score\": 0.9524020947280213\n",
      "  }\n",
      "]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coisine_array\n",
      " [[1.         0.03578923 0.12293702 ... 0.         0.         0.        ]\n",
      " [0.03578923 1.         0.08927552 ... 0.03281792 0.         0.03052111]\n",
      " [0.12293702 0.08927552 1.         ... 0.0188352  0.         0.        ]\n",
      " ...\n",
      " [0.         0.03281792 0.0188352  ... 1.         0.05063869 0.        ]\n",
      " [0.         0.         0.         ... 0.05063869 1.         0.        ]\n",
      " [0.         0.03052111 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"tfidf_misspealings+lemma\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for us to.. cont1\",\n",
      "    \"score\": 0.9961380614574664\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Stats for the day have arrived. 1 new follower and NO unfollowers happy  via\",\n",
      "    \"phrase2\": \"Stats for the day have arrived. to  new followers and NO unfollowers happy  via\",\n",
      "    \"score\": 0.9907280953459923\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy  Want this ?\",\n",
      "    \"phrase2\": \"Hey thanks for being my top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9776767776915931\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \" Tamil Nadu\",\n",
      "    \"phrase2\": \"In Tamil Nadu\",\n",
      "    \"score\": 0.9621559284566749\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy  Want this?\",\n",
      "    \"phrase2\": \"Share the love: thanks for being top new followers this week happy  Want it\",\n",
      "    \"score\": 0.9573377863269003\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9554025194926745\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9554025194926745\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \" and more. Also in epaper. \",\n",
      "    \"phrase2\": \" and more. Also in the epaper. \",\n",
      "    \"score\": 0.9548716398215152\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy\",\n",
      "    \"phrase2\": \"Share the love:thanks for being top new followers this week happy   Want this\",\n",
      "    \"score\": 0.9523397631695664\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"good morning unhappy \",\n",
      "    \"phrase2\": \"good morning\",\n",
      "    \"score\": 0.9500820285131676\n",
      "  }\n",
      "]\n",
      "\n",
      "\n",
      "coisine_array\n",
      " [[1.         0.         0.02322914 ... 0.         0.         0.        ]\n",
      " [0.         1.         0.         ... 0.         0.         0.03848839]\n",
      " [0.02322914 0.         1.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 1.         0.05189196 0.        ]\n",
      " [0.         0.         0.         ... 0.05189196 1.         0.        ]\n",
      " [0.         0.03848839 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"tfidf_filtered+tokenization\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number.. cont1\",\n",
      "    \"phrase2\": \"Hi! We tried to call your number but got no response unhappy  Please share another suitable time and an alternate number for us to.. cont1\",\n",
      "    \"score\": 0.981578319516515\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy  Want this ?\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"score\": 0.9655851292814434\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being my top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9655851292814434\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9655851292814434\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9655851292814434\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy  Want this?\",\n",
      "    \"phrase2\": \"Share the love: thanks for being top new followers this week happy\",\n",
      "    \"score\": 0.9606114473236527\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy\",\n",
      "    \"phrase2\": \"Share the love: thanks for being top new followers this week happy  Want it\",\n",
      "    \"score\": 0.9606114473236527\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy\",\n",
      "    \"phrase2\": \"Share the love:thanks for being top new followers this week happy   Want this\",\n",
      "    \"score\": 0.9606114473236527\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Wednesday.\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Wednesday. Want this\",\n",
      "    \"score\": 0.9599872519874739\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\",\n",
      "    \"score\": 0.950686408375143\n",
      "  }\n",
      "]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text_vectors in vector_columns[1:26]:\n",
    "    print_similar(text_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coisine_array\n",
      " [[1.         0.         0.02477119 ... 0.         0.         0.        ]\n",
      " [0.         1.         0.         ... 0.         0.         0.04016819]\n",
      " [0.02477119 0.         1.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 1.         0.05476263 0.        ]\n",
      " [0.         0.         0.         ... 0.05476263 1.         0.        ]\n",
      " [0.         0.04016819 0.         ... 0.         0.         1.        ]]\n",
      "text vector \"tfidf_misspealings+lemma+filtered\":\n",
      "[\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy  Want this ?\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"score\": 0.9661448929965766\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being my top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9661448929965766\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9661448929965766\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Hey thanks for being top new followers this week! Much appreciated happy\",\n",
      "    \"phrase2\": \"Hey thanks for being top new followers this week! Much appreciated happy   Want this ?\",\n",
      "    \"score\": 0.9661448929965766\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Wednesday.\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Wednesday. Want this\",\n",
      "    \"score\": 0.9617146493070419\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy  Want this?\",\n",
      "    \"phrase2\": \"Share the love: thanks for being top new followers this week happy\",\n",
      "    \"score\": 0.9613046182847013\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy\",\n",
      "    \"phrase2\": \"Share the love: thanks for being top new followers this week happy  Want it\",\n",
      "    \"score\": 0.9613046182847013\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Share the love: thanks for being top new followers this week happy\",\n",
      "    \"phrase2\": \"Share the love:thanks for being top new followers this week happy   Want this\",\n",
      "    \"score\": 0.9613046182847013\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. (Want this?\",\n",
      "    \"score\": 0.9527859882687033\n",
      "  },\n",
      "  {\n",
      "    \"phrase1\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday.\",\n",
      "    \"phrase2\": \"Thanks for the recent follow Happy to connect happy  have a great Thursday. Want this\",\n",
      "    \"score\": 0.9527859882687033\n",
      "  }\n",
      "]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text_vectors in vector_columns[26:]:\n",
    "    print_similar(text_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gyXlSntCPgG"
   },
   "source": [
    "## MachineLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "p417V3WTCXO0",
    "outputId": "f298c794-b354-45cc-8885-0f4373dd00b8"
   },
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(df['tfidf_tweet'].tolist(), df['tone'].tolist(), test_size=test_size, random_state=random_state, stratify=df['tone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMg5_ppMDC1O",
    "outputId": "44686c0d-71c4-4c9d-b376-8f21d57938b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3098, 775, 3098, 775)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_tfidf), len(X_test_tfidf), len(y_train_tfidf), len(y_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0GvC75oDDAr",
    "outputId": "6482b173-ad14-455c-9a7e-b47eae06ed61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=42)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=200, random_state=random_state)\n",
    "rf_classifier.fit(X_train_tfidf, y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "BG6mEntSDDDJ"
   },
   "outputs": [],
   "source": [
    "predictions = rf_classifier.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VaD0RQxJEVMt",
    "outputId": "f65d64e5-788e-42fe-cec1-69b3b1d3bb1c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'positive', 'negative', 'negative', 'positive',\n",
       "       'neutral', 'positive', 'positive', 'positive', 'neutral',\n",
       "       'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive',\n",
       "       'positive', 'positive', 'neutral', 'negative', 'neutral',\n",
       "       'neutral', 'neutral', 'neutral', 'positive', 'negative',\n",
       "       'negative', 'neutral', 'neutral', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'neutral', 'negative',\n",
       "       'positive', 'negative', 'negative', 'neutral', 'positive',\n",
       "       'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral',\n",
       "       'negative', 'neutral', 'neutral', 'neutral', 'positive', 'neutral',\n",
       "       'positive', 'neutral', 'negative', 'positive', 'negative',\n",
       "       'neutral', 'neutral', 'positive', 'positive', 'negative',\n",
       "       'negative', 'negative', 'negative', 'neutral', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'neutral', 'negative',\n",
       "       'negative', 'neutral', 'neutral', 'positive', 'neutral',\n",
       "       'negative', 'positive', 'positive', 'neutral', 'negative',\n",
       "       'neutral', 'positive', 'positive', 'positive', 'neutral',\n",
       "       'neutral', 'positive', 'neutral', 'positive', 'neutral',\n",
       "       'negative', 'positive', 'negative', 'negative', 'negative',\n",
       "       'negative', 'neutral', 'neutral', 'negative', 'positive',\n",
       "       'positive', 'neutral', 'negative', 'positive', 'neutral',\n",
       "       'positive', 'neutral', 'positive', 'positive', 'neutral',\n",
       "       'positive', 'negative', 'neutral', 'neutral', 'positive',\n",
       "       'neutral', 'negative', 'negative', 'negative', 'neutral',\n",
       "       'neutral', 'neutral', 'positive', 'negative', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'positive', 'positive',\n",
       "       'neutral', 'negative', 'neutral', 'positive', 'neutral', 'neutral',\n",
       "       'positive', 'neutral', 'neutral', 'neutral', 'negative', 'neutral',\n",
       "       'negative', 'positive', 'positive', 'positive', 'positive',\n",
       "       'positive', 'neutral', 'neutral', 'neutral', 'negative', 'neutral',\n",
       "       'positive', 'neutral', 'positive', 'neutral', 'positive',\n",
       "       'neutral', 'neutral', 'positive', 'negative', 'negative',\n",
       "       'neutral', 'negative', 'negative', 'negative', 'negative',\n",
       "       'positive', 'positive', 'neutral', 'neutral', 'negative',\n",
       "       'positive', 'neutral', 'neutral', 'negative', 'positive',\n",
       "       'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive',\n",
       "       'negative', 'negative', 'neutral', 'neutral', 'positive',\n",
       "       'positive', 'neutral', 'negative', 'negative', 'negative',\n",
       "       'positive', 'negative', 'neutral', 'neutral', 'negative',\n",
       "       'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral',\n",
       "       'neutral', 'positive', 'neutral', 'neutral', 'negative', 'neutral',\n",
       "       'neutral', 'negative', 'negative', 'neutral', 'neutral',\n",
       "       'negative', 'neutral', 'neutral', 'neutral', 'negative',\n",
       "       'positive', 'neutral', 'neutral', 'negative', 'neutral', 'neutral',\n",
       "       'positive', 'positive', 'neutral', 'negative', 'positive',\n",
       "       'neutral', 'positive', 'neutral', 'positive', 'neutral',\n",
       "       'negative', 'positive', 'neutral', 'neutral', 'neutral', 'neutral',\n",
       "       'neutral', 'neutral', 'neutral', 'positive', 'negative', 'neutral',\n",
       "       'neutral', 'positive', 'negative', 'positive', 'neutral',\n",
       "       'neutral', 'positive', 'positive', 'positive', 'positive',\n",
       "       'negative', 'negative', 'neutral', 'negative', 'negative',\n",
       "       'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral',\n",
       "       'neutral', 'negative', 'neutral', 'neutral', 'positive', 'neutral',\n",
       "       'positive', 'positive', 'negative', 'positive', 'negative',\n",
       "       'neutral', 'neutral', 'negative', 'neutral', 'positive',\n",
       "       'negative', 'negative', 'neutral', 'neutral', 'neutral',\n",
       "       'negative', 'neutral', 'neutral', 'positive', 'neutral',\n",
       "       'negative', 'neutral', 'negative', 'positive', 'positive',\n",
       "       'neutral', 'negative', 'neutral', 'neutral', 'neutral', 'positive',\n",
       "       'neutral', 'negative', 'neutral', 'neutral', 'negative', 'neutral',\n",
       "       'neutral', 'neutral', 'neutral', 'neutral', 'negative', 'neutral',\n",
       "       'negative', 'neutral', 'neutral', 'positive', 'neutral',\n",
       "       'negative', 'negative', 'positive', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'positive', 'positive',\n",
       "       'negative', 'neutral', 'neutral', 'negative', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'neutral', 'neutral',\n",
       "       'negative', 'neutral', 'positive', 'neutral', 'neutral',\n",
       "       'negative', 'neutral', 'neutral', 'neutral', 'negative', 'neutral',\n",
       "       'negative', 'positive', 'neutral', 'negative', 'negative',\n",
       "       'positive', 'neutral', 'negative', 'negative', 'positive',\n",
       "       'neutral', 'positive', 'negative', 'neutral', 'negative',\n",
       "       'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'neutral',\n",
       "       'negative', 'positive', 'neutral', 'neutral', 'positive',\n",
       "       'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral',\n",
       "       'negative', 'negative', 'positive', 'positive', 'negative',\n",
       "       'neutral', 'neutral', 'positive', 'positive', 'neutral', 'neutral',\n",
       "       'neutral', 'negative', 'neutral', 'neutral', 'negative',\n",
       "       'positive', 'neutral', 'neutral', 'negative', 'negative',\n",
       "       'positive', 'negative', 'positive', 'positive', 'neutral',\n",
       "       'negative', 'negative', 'negative', 'positive', 'positive',\n",
       "       'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'negative',\n",
       "       'positive', 'positive', 'positive', 'neutral', 'negative',\n",
       "       'positive', 'neutral', 'neutral', 'neutral', 'negative',\n",
       "       'negative', 'negative', 'neutral', 'negative', 'positive',\n",
       "       'neutral', 'neutral', 'negative', 'positive', 'neutral', 'neutral',\n",
       "       'neutral', 'positive', 'neutral', 'negative', 'neutral',\n",
       "       'negative', 'neutral', 'neutral', 'positive', 'positive',\n",
       "       'neutral', 'negative', 'neutral', 'neutral', 'neutral', 'neutral',\n",
       "       'positive', 'neutral', 'neutral', 'negative', 'neutral',\n",
       "       'positive', 'neutral', 'positive', 'negative', 'positive',\n",
       "       'negative', 'neutral', 'neutral', 'neutral', 'negative',\n",
       "       'positive', 'neutral', 'negative', 'negative', 'neutral',\n",
       "       'neutral', 'neutral', 'neutral', 'negative', 'positive',\n",
       "       'negative', 'negative', 'negative', 'neutral', 'neutral',\n",
       "       'positive', 'neutral', 'neutral', 'neutral', 'negative', 'neutral',\n",
       "       'positive', 'neutral', 'neutral', 'positive', 'negative',\n",
       "       'positive', 'neutral', 'negative', 'neutral', 'neutral', 'neutral',\n",
       "       'neutral', 'neutral', 'neutral', 'negative', 'negative',\n",
       "       'positive', 'neutral', 'neutral', 'negative', 'neutral',\n",
       "       'positive', 'negative', 'negative', 'negative', 'positive',\n",
       "       'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'neutral',\n",
       "       'positive', 'neutral', 'neutral', 'neutral', 'negative',\n",
       "       'positive', 'negative', 'neutral', 'neutral', 'negative',\n",
       "       'neutral', 'neutral', 'positive', 'positive', 'negative',\n",
       "       'neutral', 'neutral', 'negative', 'negative', 'positive',\n",
       "       'negative', 'positive', 'neutral', 'neutral', 'negative',\n",
       "       'negative', 'negative', 'neutral', 'positive', 'positive',\n",
       "       'negative', 'neutral', 'neutral', 'negative', 'neutral', 'neutral',\n",
       "       'neutral', 'negative', 'neutral', 'positive', 'positive',\n",
       "       'neutral', 'neutral', 'neutral', 'positive', 'positive',\n",
       "       'positive', 'negative', 'neutral', 'neutral', 'negative',\n",
       "       'positive', 'positive', 'neutral', 'neutral', 'neutral', 'neutral',\n",
       "       'neutral', 'positive', 'positive', 'negative', 'negative',\n",
       "       'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'negative',\n",
       "       'neutral', 'negative', 'negative', 'positive', 'negative',\n",
       "       'positive', 'neutral', 'neutral', 'negative', 'neutral',\n",
       "       'positive', 'positive', 'negative', 'neutral', 'neutral',\n",
       "       'negative', 'positive', 'positive', 'neutral', 'positive',\n",
       "       'negative', 'negative', 'negative', 'neutral', 'neutral',\n",
       "       'positive', 'positive', 'positive', 'positive', 'neutral',\n",
       "       'neutral', 'positive', 'neutral', 'negative', 'negative',\n",
       "       'negative', 'positive', 'negative', 'positive', 'negative',\n",
       "       'neutral', 'positive', 'negative', 'neutral', 'neutral', 'neutral',\n",
       "       'neutral', 'positive', 'positive', 'positive', 'neutral',\n",
       "       'negative', 'negative', 'neutral', 'neutral', 'neutral', 'neutral',\n",
       "       'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral',\n",
       "       'neutral', 'positive', 'negative', 'positive', 'negative',\n",
       "       'negative', 'positive', 'positive', 'neutral', 'neutral',\n",
       "       'negative', 'positive', 'neutral', 'neutral', 'negative',\n",
       "       'neutral', 'neutral', 'negative', 'positive', 'negative',\n",
       "       'positive', 'negative', 'positive', 'negative', 'neutral',\n",
       "       'negative', 'positive', 'neutral', 'positive', 'negative',\n",
       "       'neutral', 'positive', 'negative', 'neutral', 'neutral', 'neutral',\n",
       "       'neutral', 'positive', 'positive', 'negative', 'neutral',\n",
       "       'negative', 'negative', 'negative', 'positive', 'neutral',\n",
       "       'positive', 'negative', 'neutral', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'neutral', 'neutral',\n",
       "       'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral',\n",
       "       'negative', 'neutral', 'neutral', 'negative', 'negative',\n",
       "       'negative', 'positive', 'positive', 'negative', 'positive',\n",
       "       'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'negative',\n",
       "       'negative', 'positive', 'neutral', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'neutral',\n",
       "       'neutral', 'neutral', 'neutral', 'positive', 'positive',\n",
       "       'positive', 'negative', 'positive', 'neutral', 'neutral',\n",
       "       'neutral', 'positive', 'neutral', 'neutral', 'positive', 'neutral',\n",
       "       'neutral', 'negative', 'neutral'], dtype='<U8')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WO0iZ34WEccq",
    "outputId": "235fef60-5564-4d5c-e29b-2b3566146007",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.88      0.90       224\n",
      "     neutral       0.86      0.97      0.91       314\n",
      "    positive       0.94      0.81      0.87       237\n",
      "\n",
      "    accuracy                           0.90       775\n",
      "   macro avg       0.91      0.89      0.89       775\n",
      "weighted avg       0.90      0.90      0.90       775\n",
      "\n",
      "accuracy is 0.896774193548387\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_tfidf,predictions))\n",
    "print(f'accuracy is {accuracy_score(y_test_tfidf, predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x14d226e80>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=200, random_state=random_state)\n",
    "rf_classifier.fit(X_train_tfidf, y_train_tfidf)\n",
    "plot_confusion_matrix(rf_classifier, X_test_tfidf, y_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hjk61VMC510S"
   },
   "source": [
    "### All classifiers on all text vertors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "classsifiers = [\n",
    "    LogisticRegression(random_state=random_state),\n",
    "    SVC(gamma='auto'),\n",
    "    KNeighborsClassifier(n_neighbors=50),\n",
    "    DecisionTreeClassifier(random_state=random_state),\n",
    "    RandomForestClassifier(n_estimators=200, random_state=random_state),\n",
    "    GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=random_state)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ohe_tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohe_tokenization</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohe_stemming</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohe_stemming+</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohe_lemmatization</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohe_misspealings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohe_misspealings+lemma</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohe_filtered+tokenization</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohe_misspealings+lemma+filtered</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_tokenization</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_stemming</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_stemming+</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_lemmatization</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_misspealings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_misspealings+lemma</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_filtered+tokenization</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_misspealings+lemma+filtered</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_tokenization</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_stemming</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_stemming+</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_lemmatization</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_misspealings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_misspealings+lemma</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_filtered+tokenization</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_misspealings+lemma+filtered</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [ohe_tweet, ohe_tokenization, ohe_stemming, ohe_stemming+, ohe_lemmatization, ohe_misspealings, ohe_misspealings+lemma, ohe_filtered+tokenization, ohe_misspealings+lemma+filtered, wc_tweet, wc_tokenization, wc_stemming, wc_stemming+, wc_lemmatization, wc_misspealings, wc_misspealings+lemma, wc_filtered+tokenization, wc_misspealings+lemma+filtered, tfidf_tweet, tfidf_tokenization, tfidf_stemming, tfidf_stemming+, tfidf_lemmatization, tfidf_misspealings, tfidf_misspealings+lemma, tfidf_filtered+tokenization, tfidf_misspealings+lemma+filtered]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_columns = [column for column in df.columns if column.startswith((\"ohe\", \"wc\", \"tfidf\"))]\n",
    "df_scores = pd.DataFrame(index=vector_columns)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier LogisticRegression(random_state=42), vectorized text \"ohe_tweet\": accuracy is 0.5303225806451612\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"ohe_tokenization\": accuracy is 0.5329032258064517\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"ohe_stemming\": accuracy is 0.5341935483870968\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"ohe_stemming+\": accuracy is 0.5341935483870968\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"ohe_lemmatization\": accuracy is 0.5341935483870968\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"ohe_misspealings\": accuracy is 0.5329032258064517\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"ohe_misspealings+lemma\": accuracy is 0.5341935483870968\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"ohe_filtered+tokenization\": accuracy is 0.5393548387096774\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"ohe_misspealings+lemma+filtered\": accuracy is 0.5419354838709678\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"wc_tweet\": accuracy is 0.9006451612903226\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"wc_tokenization\": accuracy is 0.9019354838709678\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"wc_stemming\": accuracy is 0.9032258064516129\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"wc_stemming+\": accuracy is 0.8993548387096775\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"wc_lemmatization\": accuracy is 0.9006451612903226\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"wc_misspealings\": accuracy is 0.8929032258064517\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"wc_misspealings+lemma\": accuracy is 0.8929032258064517\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"wc_filtered+tokenization\": accuracy is 0.8851612903225806\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"wc_misspealings+lemma+filtered\": accuracy is 0.8851612903225806\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"tfidf_tweet\": accuracy is 0.8941935483870967\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"tfidf_tokenization\": accuracy is 0.896774193548387\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"tfidf_stemming\": accuracy is 0.8993548387096775\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"tfidf_stemming+\": accuracy is 0.895483870967742\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"tfidf_lemmatization\": accuracy is 0.8916129032258064\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"tfidf_misspealings\": accuracy is 0.8916129032258064\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"tfidf_misspealings+lemma\": accuracy is 0.8916129032258064\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"tfidf_filtered+tokenization\": accuracy is 0.8864516129032258\n",
      "classifier LogisticRegression(random_state=42), vectorized text \"tfidf_misspealings+lemma+filtered\": accuracy is 0.8812903225806452\n",
      "classifier SVC(gamma='auto'), vectorized text \"ohe_tweet\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"ohe_tokenization\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"ohe_stemming\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"ohe_stemming+\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"ohe_lemmatization\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"ohe_misspealings\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"ohe_misspealings+lemma\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"ohe_filtered+tokenization\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"ohe_misspealings+lemma+filtered\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"wc_tweet\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"wc_tokenization\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"wc_stemming\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"wc_stemming+\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"wc_lemmatization\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"wc_misspealings\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"wc_misspealings+lemma\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"wc_filtered+tokenization\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"wc_misspealings+lemma+filtered\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"tfidf_tweet\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"tfidf_tokenization\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"tfidf_stemming\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"tfidf_stemming+\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"tfidf_lemmatization\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"tfidf_misspealings\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"tfidf_misspealings+lemma\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"tfidf_filtered+tokenization\": accuracy is 0.40516129032258064\n",
      "classifier SVC(gamma='auto'), vectorized text \"tfidf_misspealings+lemma+filtered\": accuracy is 0.40516129032258064\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"ohe_tweet\": accuracy is 0.41935483870967744\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"ohe_tokenization\": accuracy is 0.41935483870967744\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"ohe_stemming\": accuracy is 0.41935483870967744\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"ohe_stemming+\": accuracy is 0.41935483870967744\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"ohe_lemmatization\": accuracy is 0.41935483870967744\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"ohe_misspealings\": accuracy is 0.41935483870967744\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"ohe_misspealings+lemma\": accuracy is 0.41935483870967744\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"ohe_filtered+tokenization\": accuracy is 0.42451612903225805\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"ohe_misspealings+lemma+filtered\": accuracy is 0.42451612903225805\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"wc_tweet\": accuracy is 0.6090322580645161\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"wc_tokenization\": accuracy is 0.6051612903225806\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"wc_stemming\": accuracy is 0.5883870967741935\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"wc_stemming+\": accuracy is 0.5961290322580645\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"wc_lemmatization\": accuracy is 0.5716129032258065\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"wc_misspealings\": accuracy is 0.5870967741935483\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"wc_misspealings+lemma\": accuracy is 0.5612903225806452\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"wc_filtered+tokenization\": accuracy is 0.4735483870967742\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"wc_misspealings+lemma+filtered\": accuracy is 0.47096774193548385\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"tfidf_tweet\": accuracy is 0.8129032258064516\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"tfidf_tokenization\": accuracy is 0.8180645161290323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"tfidf_stemming\": accuracy is 0.8309677419354838\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"tfidf_stemming+\": accuracy is 0.8232258064516129\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"tfidf_lemmatization\": accuracy is 0.8309677419354838\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"tfidf_misspealings\": accuracy is 0.8129032258064516\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"tfidf_misspealings+lemma\": accuracy is 0.8129032258064516\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"tfidf_filtered+tokenization\": accuracy is 0.6361290322580645\n",
      "classifier KNeighborsClassifier(n_neighbors=50), vectorized text \"tfidf_misspealings+lemma+filtered\": accuracy is 0.5703225806451613\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"ohe_tweet\": accuracy is 0.5303225806451612\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"ohe_tokenization\": accuracy is 0.5329032258064517\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"ohe_stemming\": accuracy is 0.5341935483870968\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"ohe_stemming+\": accuracy is 0.5341935483870968\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"ohe_lemmatization\": accuracy is 0.5341935483870968\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"ohe_misspealings\": accuracy is 0.5329032258064517\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"ohe_misspealings+lemma\": accuracy is 0.5341935483870968\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"ohe_filtered+tokenization\": accuracy is 0.5393548387096774\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"ohe_misspealings+lemma+filtered\": accuracy is 0.5419354838709678\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"wc_tweet\": accuracy is 0.8748387096774194\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"wc_tokenization\": accuracy is 0.8787096774193548\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"wc_stemming\": accuracy is 0.8696774193548387\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"wc_stemming+\": accuracy is 0.8658064516129033\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"wc_lemmatization\": accuracy is 0.8735483870967742\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"wc_misspealings\": accuracy is 0.8761290322580645\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"wc_misspealings+lemma\": accuracy is 0.8761290322580645\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"wc_filtered+tokenization\": accuracy is 0.8709677419354839\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"wc_misspealings+lemma+filtered\": accuracy is 0.8696774193548387\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"tfidf_tweet\": accuracy is 0.8748387096774194\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"tfidf_tokenization\": accuracy is 0.8825806451612903\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"tfidf_stemming\": accuracy is 0.8722580645161291\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"tfidf_stemming+\": accuracy is 0.8683870967741936\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"tfidf_lemmatization\": accuracy is 0.8709677419354839\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"tfidf_misspealings\": accuracy is 0.8774193548387097\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"tfidf_misspealings+lemma\": accuracy is 0.8709677419354839\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"tfidf_filtered+tokenization\": accuracy is 0.8825806451612903\n",
      "classifier DecisionTreeClassifier(random_state=42), vectorized text \"tfidf_misspealings+lemma+filtered\": accuracy is 0.8735483870967742\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"ohe_tweet\": accuracy is 0.5303225806451612\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"ohe_tokenization\": accuracy is 0.5329032258064517\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"ohe_stemming\": accuracy is 0.5341935483870968\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"ohe_stemming+\": accuracy is 0.5341935483870968\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"ohe_lemmatization\": accuracy is 0.5341935483870968\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"ohe_misspealings\": accuracy is 0.5329032258064517\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"ohe_misspealings+lemma\": accuracy is 0.5341935483870968\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"ohe_filtered+tokenization\": accuracy is 0.5393548387096774\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"ohe_misspealings+lemma+filtered\": accuracy is 0.5419354838709678\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"wc_tweet\": accuracy is 0.8929032258064517\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"wc_tokenization\": accuracy is 0.8903225806451613\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"wc_stemming\": accuracy is 0.8929032258064517\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"wc_stemming+\": accuracy is 0.8980645161290323\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"wc_lemmatization\": accuracy is 0.8903225806451613\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"wc_misspealings\": accuracy is 0.8877419354838709\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"wc_misspealings+lemma\": accuracy is 0.8877419354838709\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"wc_filtered+tokenization\": accuracy is 0.8864516129032258\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"wc_misspealings+lemma+filtered\": accuracy is 0.8890322580645161\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"tfidf_tweet\": accuracy is 0.896774193548387\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"tfidf_tokenization\": accuracy is 0.9006451612903226\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"tfidf_stemming\": accuracy is 0.896774193548387\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"tfidf_stemming+\": accuracy is 0.895483870967742\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"tfidf_lemmatization\": accuracy is 0.8941935483870967\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"tfidf_misspealings\": accuracy is 0.8980645161290323\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"tfidf_misspealings+lemma\": accuracy is 0.8993548387096775\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"tfidf_filtered+tokenization\": accuracy is 0.8903225806451613\n",
      "classifier RandomForestClassifier(n_estimators=200, random_state=42), vectorized text \"tfidf_misspealings+lemma+filtered\": accuracy is 0.8864516129032258\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"ohe_tweet\": accuracy is 0.49548387096774194\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"ohe_tokenization\": accuracy is 0.49548387096774194\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"ohe_stemming\": accuracy is 0.49548387096774194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"ohe_stemming+\": accuracy is 0.4967741935483871\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"ohe_lemmatization\": accuracy is 0.49548387096774194\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"ohe_misspealings\": accuracy is 0.49548387096774194\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"ohe_misspealings+lemma\": accuracy is 0.49290322580645163\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"ohe_filtered+tokenization\": accuracy is 0.4993548387096774\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"ohe_misspealings+lemma+filtered\": accuracy is 0.5006451612903225\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"wc_tweet\": accuracy is 0.8787096774193548\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"wc_tokenization\": accuracy is 0.8787096774193548\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"wc_stemming\": accuracy is 0.8825806451612903\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"wc_stemming+\": accuracy is 0.8838709677419355\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"wc_lemmatization\": accuracy is 0.8825806451612903\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"wc_misspealings\": accuracy is 0.8787096774193548\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"wc_misspealings+lemma\": accuracy is 0.8825806451612903\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"wc_filtered+tokenization\": accuracy is 0.8554838709677419\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"wc_misspealings+lemma+filtered\": accuracy is 0.8619354838709677\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"tfidf_tweet\": accuracy is 0.8748387096774194\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"tfidf_tokenization\": accuracy is 0.8748387096774194\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"tfidf_stemming\": accuracy is 0.8670967741935484\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"tfidf_stemming+\": accuracy is 0.863225806451613\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"tfidf_lemmatization\": accuracy is 0.88\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"tfidf_misspealings\": accuracy is 0.88\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"tfidf_misspealings+lemma\": accuracy is 0.8670967741935484\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"tfidf_filtered+tokenization\": accuracy is 0.8593548387096774\n",
      "classifier GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42), vectorized text \"tfidf_misspealings+lemma+filtered\": accuracy is 0.8567741935483871\n"
     ]
    }
   ],
   "source": [
    "for classifier in classsifiers:\n",
    "    list_scores = []\n",
    "    for vectorized_text in vector_columns:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df[vectorized_text].tolist(), df['tone'].tolist(), test_size=test_size, random_state=random_state, stratify=df['tone'])\n",
    "        classifier.fit(X_train, y_train)\n",
    "        predictions = classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        list_scores.append(accuracy)\n",
    "        print(f'classifier {classifier}, vectorized text \"{vectorized_text}\": accuracy is {accuracy}')\n",
    "    df_scores[str(classifier)] = list_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisticRegression(random_state=42)</th>\n",
       "      <th>SVC(gamma='auto')</th>\n",
       "      <th>KNeighborsClassifier(n_neighbors=50)</th>\n",
       "      <th>DecisionTreeClassifier(random_state=42)</th>\n",
       "      <th>RandomForestClassifier(n_estimators=200, random_state=42)</th>\n",
       "      <th>GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ohe_tweet</th>\n",
       "      <td>0.530323</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.530323</td>\n",
       "      <td>0.530323</td>\n",
       "      <td>0.495484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohe_tokenization</th>\n",
       "      <td>0.532903</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.532903</td>\n",
       "      <td>0.532903</td>\n",
       "      <td>0.495484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohe_stemming</th>\n",
       "      <td>0.534194</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.534194</td>\n",
       "      <td>0.534194</td>\n",
       "      <td>0.495484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohe_stemming+</th>\n",
       "      <td>0.534194</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.534194</td>\n",
       "      <td>0.534194</td>\n",
       "      <td>0.496774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohe_lemmatization</th>\n",
       "      <td>0.534194</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.534194</td>\n",
       "      <td>0.534194</td>\n",
       "      <td>0.495484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohe_misspealings</th>\n",
       "      <td>0.532903</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.532903</td>\n",
       "      <td>0.532903</td>\n",
       "      <td>0.495484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohe_misspealings+lemma</th>\n",
       "      <td>0.534194</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.534194</td>\n",
       "      <td>0.534194</td>\n",
       "      <td>0.492903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohe_filtered+tokenization</th>\n",
       "      <td>0.539355</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.424516</td>\n",
       "      <td>0.539355</td>\n",
       "      <td>0.539355</td>\n",
       "      <td>0.499355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohe_misspealings+lemma+filtered</th>\n",
       "      <td>0.541935</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.424516</td>\n",
       "      <td>0.541935</td>\n",
       "      <td>0.541935</td>\n",
       "      <td>0.500645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_tweet</th>\n",
       "      <td>0.900645</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.609032</td>\n",
       "      <td>0.874839</td>\n",
       "      <td>0.892903</td>\n",
       "      <td>0.878710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_tokenization</th>\n",
       "      <td>0.901935</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.605161</td>\n",
       "      <td>0.878710</td>\n",
       "      <td>0.890323</td>\n",
       "      <td>0.878710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_stemming</th>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.588387</td>\n",
       "      <td>0.869677</td>\n",
       "      <td>0.892903</td>\n",
       "      <td>0.882581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_stemming+</th>\n",
       "      <td>0.899355</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.596129</td>\n",
       "      <td>0.865806</td>\n",
       "      <td>0.898065</td>\n",
       "      <td>0.883871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_lemmatization</th>\n",
       "      <td>0.900645</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.571613</td>\n",
       "      <td>0.873548</td>\n",
       "      <td>0.890323</td>\n",
       "      <td>0.882581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_misspealings</th>\n",
       "      <td>0.892903</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.587097</td>\n",
       "      <td>0.876129</td>\n",
       "      <td>0.887742</td>\n",
       "      <td>0.878710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_misspealings+lemma</th>\n",
       "      <td>0.892903</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.561290</td>\n",
       "      <td>0.876129</td>\n",
       "      <td>0.887742</td>\n",
       "      <td>0.882581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_filtered+tokenization</th>\n",
       "      <td>0.885161</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.473548</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.886452</td>\n",
       "      <td>0.855484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wc_misspealings+lemma+filtered</th>\n",
       "      <td>0.885161</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.470968</td>\n",
       "      <td>0.869677</td>\n",
       "      <td>0.889032</td>\n",
       "      <td>0.861935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_tweet</th>\n",
       "      <td>0.894194</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.812903</td>\n",
       "      <td>0.874839</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.874839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_tokenization</th>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.818065</td>\n",
       "      <td>0.882581</td>\n",
       "      <td>0.900645</td>\n",
       "      <td>0.874839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_stemming</th>\n",
       "      <td>0.899355</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.830968</td>\n",
       "      <td>0.872258</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.867097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_stemming+</th>\n",
       "      <td>0.895484</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.823226</td>\n",
       "      <td>0.868387</td>\n",
       "      <td>0.895484</td>\n",
       "      <td>0.863226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_lemmatization</th>\n",
       "      <td>0.891613</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.830968</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.894194</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_misspealings</th>\n",
       "      <td>0.891613</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.812903</td>\n",
       "      <td>0.877419</td>\n",
       "      <td>0.898065</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_misspealings+lemma</th>\n",
       "      <td>0.891613</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.812903</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.899355</td>\n",
       "      <td>0.867097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_filtered+tokenization</th>\n",
       "      <td>0.886452</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.636129</td>\n",
       "      <td>0.882581</td>\n",
       "      <td>0.890323</td>\n",
       "      <td>0.859355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_misspealings+lemma+filtered</th>\n",
       "      <td>0.881290</td>\n",
       "      <td>0.405161</td>\n",
       "      <td>0.570323</td>\n",
       "      <td>0.873548</td>\n",
       "      <td>0.886452</td>\n",
       "      <td>0.856774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   LogisticRegression(random_state=42)  \\\n",
       "ohe_tweet                                                     0.530323   \n",
       "ohe_tokenization                                              0.532903   \n",
       "ohe_stemming                                                  0.534194   \n",
       "ohe_stemming+                                                 0.534194   \n",
       "ohe_lemmatization                                             0.534194   \n",
       "ohe_misspealings                                              0.532903   \n",
       "ohe_misspealings+lemma                                        0.534194   \n",
       "ohe_filtered+tokenization                                     0.539355   \n",
       "ohe_misspealings+lemma+filtered                               0.541935   \n",
       "wc_tweet                                                      0.900645   \n",
       "wc_tokenization                                               0.901935   \n",
       "wc_stemming                                                   0.903226   \n",
       "wc_stemming+                                                  0.899355   \n",
       "wc_lemmatization                                              0.900645   \n",
       "wc_misspealings                                               0.892903   \n",
       "wc_misspealings+lemma                                         0.892903   \n",
       "wc_filtered+tokenization                                      0.885161   \n",
       "wc_misspealings+lemma+filtered                                0.885161   \n",
       "tfidf_tweet                                                   0.894194   \n",
       "tfidf_tokenization                                            0.896774   \n",
       "tfidf_stemming                                                0.899355   \n",
       "tfidf_stemming+                                               0.895484   \n",
       "tfidf_lemmatization                                           0.891613   \n",
       "tfidf_misspealings                                            0.891613   \n",
       "tfidf_misspealings+lemma                                      0.891613   \n",
       "tfidf_filtered+tokenization                                   0.886452   \n",
       "tfidf_misspealings+lemma+filtered                             0.881290   \n",
       "\n",
       "                                   SVC(gamma='auto')  \\\n",
       "ohe_tweet                                   0.405161   \n",
       "ohe_tokenization                            0.405161   \n",
       "ohe_stemming                                0.405161   \n",
       "ohe_stemming+                               0.405161   \n",
       "ohe_lemmatization                           0.405161   \n",
       "ohe_misspealings                            0.405161   \n",
       "ohe_misspealings+lemma                      0.405161   \n",
       "ohe_filtered+tokenization                   0.405161   \n",
       "ohe_misspealings+lemma+filtered             0.405161   \n",
       "wc_tweet                                    0.405161   \n",
       "wc_tokenization                             0.405161   \n",
       "wc_stemming                                 0.405161   \n",
       "wc_stemming+                                0.405161   \n",
       "wc_lemmatization                            0.405161   \n",
       "wc_misspealings                             0.405161   \n",
       "wc_misspealings+lemma                       0.405161   \n",
       "wc_filtered+tokenization                    0.405161   \n",
       "wc_misspealings+lemma+filtered              0.405161   \n",
       "tfidf_tweet                                 0.405161   \n",
       "tfidf_tokenization                          0.405161   \n",
       "tfidf_stemming                              0.405161   \n",
       "tfidf_stemming+                             0.405161   \n",
       "tfidf_lemmatization                         0.405161   \n",
       "tfidf_misspealings                          0.405161   \n",
       "tfidf_misspealings+lemma                    0.405161   \n",
       "tfidf_filtered+tokenization                 0.405161   \n",
       "tfidf_misspealings+lemma+filtered           0.405161   \n",
       "\n",
       "                                   KNeighborsClassifier(n_neighbors=50)  \\\n",
       "ohe_tweet                                                      0.419355   \n",
       "ohe_tokenization                                               0.419355   \n",
       "ohe_stemming                                                   0.419355   \n",
       "ohe_stemming+                                                  0.419355   \n",
       "ohe_lemmatization                                              0.419355   \n",
       "ohe_misspealings                                               0.419355   \n",
       "ohe_misspealings+lemma                                         0.419355   \n",
       "ohe_filtered+tokenization                                      0.424516   \n",
       "ohe_misspealings+lemma+filtered                                0.424516   \n",
       "wc_tweet                                                       0.609032   \n",
       "wc_tokenization                                                0.605161   \n",
       "wc_stemming                                                    0.588387   \n",
       "wc_stemming+                                                   0.596129   \n",
       "wc_lemmatization                                               0.571613   \n",
       "wc_misspealings                                                0.587097   \n",
       "wc_misspealings+lemma                                          0.561290   \n",
       "wc_filtered+tokenization                                       0.473548   \n",
       "wc_misspealings+lemma+filtered                                 0.470968   \n",
       "tfidf_tweet                                                    0.812903   \n",
       "tfidf_tokenization                                             0.818065   \n",
       "tfidf_stemming                                                 0.830968   \n",
       "tfidf_stemming+                                                0.823226   \n",
       "tfidf_lemmatization                                            0.830968   \n",
       "tfidf_misspealings                                             0.812903   \n",
       "tfidf_misspealings+lemma                                       0.812903   \n",
       "tfidf_filtered+tokenization                                    0.636129   \n",
       "tfidf_misspealings+lemma+filtered                              0.570323   \n",
       "\n",
       "                                   DecisionTreeClassifier(random_state=42)  \\\n",
       "ohe_tweet                                                         0.530323   \n",
       "ohe_tokenization                                                  0.532903   \n",
       "ohe_stemming                                                      0.534194   \n",
       "ohe_stemming+                                                     0.534194   \n",
       "ohe_lemmatization                                                 0.534194   \n",
       "ohe_misspealings                                                  0.532903   \n",
       "ohe_misspealings+lemma                                            0.534194   \n",
       "ohe_filtered+tokenization                                         0.539355   \n",
       "ohe_misspealings+lemma+filtered                                   0.541935   \n",
       "wc_tweet                                                          0.874839   \n",
       "wc_tokenization                                                   0.878710   \n",
       "wc_stemming                                                       0.869677   \n",
       "wc_stemming+                                                      0.865806   \n",
       "wc_lemmatization                                                  0.873548   \n",
       "wc_misspealings                                                   0.876129   \n",
       "wc_misspealings+lemma                                             0.876129   \n",
       "wc_filtered+tokenization                                          0.870968   \n",
       "wc_misspealings+lemma+filtered                                    0.869677   \n",
       "tfidf_tweet                                                       0.874839   \n",
       "tfidf_tokenization                                                0.882581   \n",
       "tfidf_stemming                                                    0.872258   \n",
       "tfidf_stemming+                                                   0.868387   \n",
       "tfidf_lemmatization                                               0.870968   \n",
       "tfidf_misspealings                                                0.877419   \n",
       "tfidf_misspealings+lemma                                          0.870968   \n",
       "tfidf_filtered+tokenization                                       0.882581   \n",
       "tfidf_misspealings+lemma+filtered                                 0.873548   \n",
       "\n",
       "                                   RandomForestClassifier(n_estimators=200, random_state=42)  \\\n",
       "ohe_tweet                                                                   0.530323           \n",
       "ohe_tokenization                                                            0.532903           \n",
       "ohe_stemming                                                                0.534194           \n",
       "ohe_stemming+                                                               0.534194           \n",
       "ohe_lemmatization                                                           0.534194           \n",
       "ohe_misspealings                                                            0.532903           \n",
       "ohe_misspealings+lemma                                                      0.534194           \n",
       "ohe_filtered+tokenization                                                   0.539355           \n",
       "ohe_misspealings+lemma+filtered                                             0.541935           \n",
       "wc_tweet                                                                    0.892903           \n",
       "wc_tokenization                                                             0.890323           \n",
       "wc_stemming                                                                 0.892903           \n",
       "wc_stemming+                                                                0.898065           \n",
       "wc_lemmatization                                                            0.890323           \n",
       "wc_misspealings                                                             0.887742           \n",
       "wc_misspealings+lemma                                                       0.887742           \n",
       "wc_filtered+tokenization                                                    0.886452           \n",
       "wc_misspealings+lemma+filtered                                              0.889032           \n",
       "tfidf_tweet                                                                 0.896774           \n",
       "tfidf_tokenization                                                          0.900645           \n",
       "tfidf_stemming                                                              0.896774           \n",
       "tfidf_stemming+                                                             0.895484           \n",
       "tfidf_lemmatization                                                         0.894194           \n",
       "tfidf_misspealings                                                          0.898065           \n",
       "tfidf_misspealings+lemma                                                    0.899355           \n",
       "tfidf_filtered+tokenization                                                 0.890323           \n",
       "tfidf_misspealings+lemma+filtered                                           0.886452           \n",
       "\n",
       "                                   GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42)  \n",
       "ohe_tweet                                                                   0.495484                            \n",
       "ohe_tokenization                                                            0.495484                            \n",
       "ohe_stemming                                                                0.495484                            \n",
       "ohe_stemming+                                                               0.496774                            \n",
       "ohe_lemmatization                                                           0.495484                            \n",
       "ohe_misspealings                                                            0.495484                            \n",
       "ohe_misspealings+lemma                                                      0.492903                            \n",
       "ohe_filtered+tokenization                                                   0.499355                            \n",
       "ohe_misspealings+lemma+filtered                                             0.500645                            \n",
       "wc_tweet                                                                    0.878710                            \n",
       "wc_tokenization                                                             0.878710                            \n",
       "wc_stemming                                                                 0.882581                            \n",
       "wc_stemming+                                                                0.883871                            \n",
       "wc_lemmatization                                                            0.882581                            \n",
       "wc_misspealings                                                             0.878710                            \n",
       "wc_misspealings+lemma                                                       0.882581                            \n",
       "wc_filtered+tokenization                                                    0.855484                            \n",
       "wc_misspealings+lemma+filtered                                              0.861935                            \n",
       "tfidf_tweet                                                                 0.874839                            \n",
       "tfidf_tokenization                                                          0.874839                            \n",
       "tfidf_stemming                                                              0.867097                            \n",
       "tfidf_stemming+                                                             0.863226                            \n",
       "tfidf_lemmatization                                                         0.880000                            \n",
       "tfidf_misspealings                                                          0.880000                            \n",
       "tfidf_misspealings+lemma                                                    0.867097                            \n",
       "tfidf_filtered+tokenization                                                 0.859355                            \n",
       "tfidf_misspealings+lemma+filtered                                           0.856774                            "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_gs = ['wc_tokenization', 'tfidf_tokenization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "best params set is {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "vectorized text \"wc_tokenization\", accuracy is 0.8916129032258064\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "best params set is {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "vectorized text \"tfidf_tokenization\", accuracy is 0.8980645161290323\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [200, 300]\n",
    "max_depth = [5, 8, None]\n",
    "min_samples_split = [2, 5]\n",
    "min_samples_leaf = [1, 2] \n",
    "params = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "             min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "\n",
    "for vectors in vectors_gs:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[vectors].tolist(), df['tone'].tolist(), test_size=test_size, random_state=random_state, stratify=df['tone'])\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    gridF = GridSearchCV(rf, params, cv = 3, verbose=1)\n",
    "    bestF = gridF.fit(X_train, y_train)\n",
    "    print(f'best params set is {bestF.best_params_}')\n",
    "    best_predictions = bestF.predict(X_test)\n",
    "    print(f'vectorized text \"{vectors}\", accuracy is {accuracy_score(y_test, best_predictions)}')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params set is {'C': 10, 'penalty': 'l2'}\n",
      "vectorized text \"tfidf_misspealings+lemma+filtered\", accuracy is 0.9019354838709678\n"
     ]
    }
   ],
   "source": [
    "penalty = ['l2']\n",
    "C = [0.1, 1, 10]\n",
    "\n",
    "parameters = dict(penalty = penalty, C = C)\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['wc_tokenization'].tolist(), df['tone'].tolist(), test_size=test_size, random_state=random_state, stratify=df['tone'])\n",
    "\n",
    "grid_search = GridSearchCV(estimator=lr, param_grid=parameters, scoring='accuracy', cv=3)\n",
    "best_lr = grid_search.fit(X_train, y_train)\n",
    "print(f'best params set is {best_lr.best_params_}')\n",
    "best_predictions = best_lr.predict(X_test)\n",
    "print(f'vectorized text \"{vectorized_text}\", accuracy is {accuracy_score(y_test, best_predictions)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vectors = CountVectorizer().fit_transform(df['filtered+tokenization'].astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=3)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=3)\n",
    "lda.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "/Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/marina.romashkova/.pyenv/versions/3.9.5/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer().fit(df['filtered+tokenization'].astype('str'))\n",
    "p3 = pyLDAvis.sklearn.prepare(lda, vectors, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el708256327843047316624350\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el708256327843047316624350_data = {\"mdsDat\": {\"x\": [-0.10830929367164298, -0.05572413187562406, 0.16403342554726696], \"y\": [0.10425925959692235, -0.12920716172566254, 0.02494790212874021], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [39.711651228143204, 33.06496213542327, 27.22338663643353]}, \"tinfo\": {\"Term\": [\"happy\", \"great\", \"follow\", \"also\", \"recent\", \"people\", \"court\", \"epaper\", \"sad\", \"thursday\", \"thanks\", \"crying\", \"miss\", \"connect\", \"govt\", \"week\", \"top\", \"supreme\", \"new\", \"smile\", \"today\", \"case\", \"still\", \"bjp\", \"time\", \"thank\", \"us\", \"unhappy\", \"says\", \"much\", \"great\", \"follow\", \"recent\", \"thursday\", \"connect\", \"miss\", \"crying\", \"still\", \"joy\", \"thank\", \"school\", \"dying\", \"first\", \"morning\", \"thirst\", \"dm\", \"koalas\", \"stream\", \"home\", \"long\", \"never\", \"tomorrow\", \"things\", \"genie\", \"tweets\", \"best\", \"number\", \"yeah\", \"reading\", \"nayar\", \"time\", \"sorry\", \"us\", \"could\", \"happy\", \"thanks\", \"back\", \"really\", \"like\", \"unhappy\", \"want\", \"day\", \"much\", \"good\", \"get\", \"please\", \"ll\", \"appreciated\", \"india\", \"one\", \"love\", \"also\", \"epaper\", \"sad\", \"week\", \"people\", \"top\", \"followers\", \"damn\", \"definitely\", \"members\", \"let\", \"old\", \"mutual\", \"sure\", \"attend\", \"sincerely\", \"theres\", \"arms\", \"bcs\", \"bh\", \"community\", \"feel\", \"hey\", \"tonight\", \"wants\", \"engaged\", \"vote\", \"twitter\", \"try\", \"killed\", \"bengal\", \"smile\", \"year\", \"party\", \"today\", \"ca\", \"says\", \"new\", \"make\", \"fun\", \"unhappy\", \"love\", \"one\", \"happy\", \"want\", \"re\", \"much\", \"thanks\", \"get\", \"birthday\", \"court\", \"govt\", \"supreme\", \"bjp\", \"keep\", \"trump\", \"kisses\", \"case\", \"thefashionicon\", \"holding\", \"sent\", \"mcd\", \"judge\", \"others\", \"beautiful\", \"anyways\", \"trophy\", \"yg\", \"asks\", \"probably\", \"plea\", \"years\", \"driver\", \"hc\", \"tells\", \"trial\", \"york\", \"mp\", \"pradesh\", \"governor\", \"police\", \"arrested\", \"said\", \"minister\", \"going\", \"see\", \"live\", \"chief\", \"go\", \"gets\", \"modi\", \"unhappy\", \"india\", \"attack\", \"new\", \"ll\", \"since\", \"cm\", \"meet\", \"happy\"], \"Freq\": [610.0, 105.0, 100.0, 71.0, 77.0, 64.0, 50.0, 58.0, 58.0, 65.0, 144.0, 60.0, 59.0, 58.0, 38.0, 44.0, 39.0, 30.0, 62.0, 47.0, 58.0, 26.0, 35.0, 23.0, 42.0, 35.0, 49.0, 602.0, 46.0, 85.0, 105.45723710314928, 99.99852473972953, 76.63356487770889, 64.94312949644261, 57.92983842693063, 59.471782099900594, 60.234132351088306, 35.303469517784, 27.52341040791371, 34.94942464943118, 18.94638549445405, 17.368728351643437, 15.053484734137635, 14.28409024988783, 13.507539640675976, 13.507491303992573, 12.728157009803807, 14.942449315757028, 13.445250193068606, 11.936078785687544, 23.113716579788186, 12.611412202305928, 10.361199860070803, 9.611421790796209, 9.58621449136774, 14.010574817964669, 8.827400814910051, 8.820242151468266, 8.81786514478211, 8.805476824536616, 40.334841987994814, 16.744790532101693, 44.446780965358876, 28.888940307858125, 471.8790036994969, 111.81171810855406, 29.3777627089459, 34.26172090536151, 55.55275475097735, 337.4789900018871, 83.45471340735628, 34.18285967433975, 54.42485205035561, 40.25801348109745, 44.087740355102625, 34.395891411046264, 27.342879660538326, 23.002915630092613, 25.283236590568077, 22.739362398522214, 23.181835654499533, 71.3154722210349, 58.33503092696129, 57.47558693921152, 43.80262756578761, 64.10297549436741, 39.1903159617082, 20.879072279846525, 20.12236934577595, 17.06462636083539, 17.035385202811756, 19.985407896694237, 16.281778461690617, 15.54009341117926, 15.523192884497726, 14.771896700012325, 14.011887403632834, 14.011885714220925, 14.005749171983364, 13.247604197050451, 13.247604197050451, 12.480168230025319, 16.140045907563074, 16.83787862592495, 12.434829016993666, 12.428722420348093, 10.95407256394347, 12.39996995530501, 11.62755469284608, 23.23521505961825, 8.642762943009146, 18.598690602901595, 41.66438910310668, 20.123054134363258, 21.83659491591052, 46.56262930319778, 35.06608822574975, 36.25720229882396, 44.749360820014346, 31.6018982441455, 21.496969857087393, 193.92892131162452, 40.56664315830777, 31.68645116180387, 128.37280223733612, 49.96508317449849, 22.670003761049504, 30.44225354332685, 32.24516764751609, 20.7744231414491, 18.496700149662814, 49.602699969459856, 38.357521601256096, 30.157338574497203, 23.302235402270252, 19.657636061839206, 16.69023139232775, 13.693365689884544, 25.19562675387498, 12.944780524474318, 10.707347244863785, 10.686012361681295, 9.966459145515206, 9.95463583413648, 9.950492171344928, 9.930897804847046, 8.475777505779917, 8.475777505779917, 8.475777505779917, 9.170165856480814, 8.456683970746147, 8.449420357970475, 8.444026648482277, 7.727749915374808, 7.726033570971173, 7.7251185493275685, 7.725118919022714, 7.719966799959826, 7.71881119676957, 7.715891565502661, 10.494836257188565, 13.262889081183078, 8.92401940686871, 14.090066211027663, 22.54502177998391, 25.23345994917327, 27.005900880223603, 12.268509994459691, 14.639703904927835, 21.34339038969432, 11.268585813827052, 12.184440597563887, 71.22191321303761, 21.047755580849135, 9.678476684142305, 17.278154311921018, 13.122423181269864, 10.713432262270477, 10.784856416094081, 10.391168880683145, 10.131242717528584], \"Total\": [610.0, 105.0, 100.0, 71.0, 77.0, 64.0, 50.0, 58.0, 58.0, 65.0, 144.0, 60.0, 59.0, 58.0, 38.0, 44.0, 39.0, 30.0, 62.0, 47.0, 58.0, 26.0, 35.0, 23.0, 42.0, 35.0, 49.0, 602.0, 46.0, 85.0, 105.97322033015672, 100.51781328267653, 77.13832331730947, 65.44834129896162, 58.43437087189642, 59.99269551265842, 60.77135358643525, 35.83316024207332, 28.040166889775836, 35.818596903857575, 19.46735097545754, 17.907689326384414, 15.570613580775271, 14.791704444236482, 14.012455659782962, 14.012452735846725, 13.23312242930943, 15.566315999159587, 14.010293691176557, 12.453388209845125, 24.12291126942248, 13.228765907478643, 10.89442138684558, 10.115813910415921, 10.115270176027277, 14.7862378612885, 9.336293814336518, 9.336155091431774, 9.336051105046112, 9.335472061338551, 42.807991858709634, 17.89636298465286, 49.76677330317819, 31.834941785013992, 610.3830486543616, 144.31071938392924, 34.10238900444317, 41.92720286154124, 73.65851232966898, 602.6298245265492, 142.79935803621228, 46.511286635564055, 85.11714020761761, 61.096995946961954, 70.28275173780531, 53.1781436053379, 40.74762437582558, 34.067276499233365, 55.820129611152105, 54.69780896289161, 73.60373210170962, 71.84348628336356, 58.85037651185398, 58.086937841018816, 44.329081063512426, 64.9574870448822, 39.7426133130371, 21.400081501694977, 20.635585896703727, 17.578406292113662, 17.579019010903583, 20.63822265992393, 16.813703013557344, 16.049785182616112, 16.049729319093256, 15.285569389137905, 14.521190788397181, 14.52119094607113, 14.521080246114426, 13.756893783532105, 13.756893783532105, 12.992640034477414, 16.810694626274294, 17.582777102753063, 12.992604992545361, 12.992378278507795, 11.463996297151706, 12.991081737443151, 12.229685199854284, 24.470911297523546, 9.171376544795152, 19.855000903306422, 47.433945524500075, 22.131843075373858, 24.497309836602604, 58.23944956859152, 44.49019741061079, 46.41753086045368, 62.298903374616756, 41.44943440611896, 26.063885120349106, 602.6298245265492, 73.60373210170962, 54.69780896289161, 610.3830486543616, 142.79935803621228, 39.28911283567624, 85.11714020761761, 144.31071938392924, 70.28275173780531, 33.91612511360327, 50.12577763203545, 38.90811636148716, 30.680227158240157, 23.954280509877197, 20.210470854395066, 17.218195530134235, 14.226671121626392, 26.212645241611398, 13.47878871412957, 11.234796599092572, 11.235545765131493, 10.486798518233725, 10.487235844616574, 10.487328751364812, 10.487881313297937, 8.990815217907365, 8.990815217907365, 8.990815217907365, 9.740082085975803, 8.991616112023763, 8.991447711459744, 8.991865299376329, 8.242916473644232, 8.242948316940423, 8.242975456348276, 8.2429819523742, 8.243242040156817, 8.243277593857176, 8.243390292553686, 11.241131467076425, 14.23658302818051, 9.745358671659417, 17.276964611534098, 31.775841799230435, 37.11726150417082, 47.08878286673511, 15.845081254943155, 20.421624764518835, 39.55868040785752, 14.28007213439893, 18.081303895803366, 602.6298245265492, 55.820129611152105, 12.021685539804299, 62.298903374616756, 40.74762437582558, 18.96998141812371, 22.69803649560872, 17.435401346648522, 610.3830486543616], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.3528, -4.4059, -4.672, -4.8376, -4.9518, -4.9256, -4.9128, -5.4471, -5.696, -5.4572, -6.0695, -6.1564, -6.2995, -6.3519, -6.4078, -6.4078, -6.4673, -6.3069, -6.4124, -6.5315, -5.8706, -6.4765, -6.673, -6.7481, -6.7507, -6.3713, -6.8332, -6.834, -6.8343, -6.8357, -5.3139, -6.193, -5.2168, -5.6476, -2.8544, -4.2943, -5.6308, -5.477, -4.9937, -3.1896, -4.5868, -5.4793, -5.0143, -5.3158, -5.2249, -5.4731, -5.7026, -5.8755, -5.7809, -5.887, -5.8677, -4.5608, -4.7617, -4.7765, -5.0482, -4.6674, -5.1595, -5.7892, -5.8261, -5.9909, -5.9926, -5.8329, -6.0379, -6.0845, -6.0856, -6.1352, -6.188, -6.188, -6.1884, -6.2441, -6.2441, -6.3038, -6.0466, -6.0043, -6.3074, -6.3079, -6.4342, -6.3102, -6.3745, -5.6822, -6.6712, -5.9048, -5.0983, -5.826, -5.7443, -4.9871, -5.2707, -5.2373, -5.0268, -5.3747, -5.76, -3.5604, -5.125, -5.372, -3.973, -4.9166, -5.7069, -5.4121, -5.3545, -5.7942, -5.9103, -4.7295, -4.9866, -5.2271, -5.485, -5.655, -5.8187, -6.0166, -5.4068, -6.0728, -6.2626, -6.2646, -6.3343, -6.3355, -6.3359, -6.3379, -6.4963, -6.4963, -6.4963, -6.4175, -6.4985, -6.4994, -6.5, -6.5887, -6.5889, -6.589, -6.589, -6.5897, -6.5898, -6.5902, -6.2826, -6.0485, -6.4448, -5.988, -5.518, -5.4053, -5.3374, -6.1265, -5.9498, -5.5728, -6.2115, -6.1333, -4.3677, -5.5867, -6.3636, -5.7841, -6.0592, -6.262, -6.2554, -6.2925, -6.3179], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9186, 0.9183, 0.917, 0.9158, 0.9149, 0.9148, 0.9146, 0.9086, 0.9049, 0.899, 0.8964, 0.893, 0.8897, 0.8886, 0.8868, 0.8868, 0.8846, 0.8826, 0.8824, 0.8811, 0.8808, 0.8757, 0.8733, 0.8724, 0.8698, 0.8696, 0.8675, 0.8667, 0.8664, 0.8651, 0.864, 0.857, 0.8105, 0.8264, 0.6662, 0.6684, 0.7744, 0.7216, 0.6414, 0.3437, 0.3864, 0.6156, 0.4763, 0.5064, 0.4572, 0.4878, 0.5246, 0.5308, 0.1315, 0.0458, -0.2318, 1.0993, 1.0979, 1.0961, 1.0947, 1.0935, 1.0927, 1.082, 1.0815, 1.077, 1.0753, 1.0746, 1.0745, 1.0744, 1.0733, 1.0725, 1.071, 1.071, 1.0706, 1.069, 1.069, 1.0665, 1.066, 1.0634, 1.0628, 1.0623, 1.0612, 1.0601, 1.0562, 1.0549, 1.0473, 1.0413, 0.977, 1.0115, 0.9917, 0.8829, 0.8687, 0.8597, 0.7758, 0.8354, 0.9141, -0.0271, 0.5109, 0.5608, -0.4525, 0.0566, 0.5568, 0.0785, -0.3919, -0.1121, 0.5004, 1.2906, 1.2868, 1.2839, 1.2735, 1.2734, 1.27, 1.2629, 1.2615, 1.2607, 1.253, 1.2509, 1.2502, 1.249, 1.2485, 1.2465, 1.2421, 1.2421, 1.2421, 1.2408, 1.2398, 1.2389, 1.2382, 1.2366, 1.2363, 1.2362, 1.2362, 1.2355, 1.2354, 1.235, 1.2324, 1.2302, 1.213, 1.0972, 0.9579, 0.9152, 0.7451, 1.0453, 0.9682, 0.6841, 1.0642, 0.9064, -0.8344, 0.3258, 1.0843, 0.0186, 0.168, 0.7297, 0.557, 0.7835, -2.7974]}, \"token.table\": {\"Topic\": [2, 3, 1, 2, 2, 2, 3, 3, 2, 3, 2, 1, 3, 2, 3, 2, 3, 1, 2, 2, 1, 2, 3, 1, 2, 3, 1, 3, 1, 3, 2, 3, 2, 1, 1, 3, 3, 1, 2, 1, 2, 2, 1, 3, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 3, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 3, 3, 1, 1, 2, 3, 3, 2, 3, 1, 1, 2, 3, 1, 3, 3, 2, 3, 1, 2, 1, 2, 3, 1, 3, 1, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 1, 3, 1, 2, 3, 1, 3, 1, 2, 2, 1, 1, 3, 2, 3, 1, 2, 1, 2, 3, 1, 2, 2, 3, 3, 1, 2, 3, 2, 3, 3, 3, 1, 2, 1, 1, 2, 1, 2, 2, 3, 2, 3, 1, 1, 2, 3, 3, 1, 2, 3, 2, 1, 2, 3, 1, 2, 1, 1, 3, 2, 3, 1, 3, 1, 2, 3, 2, 1, 1, 1, 1, 2, 1, 2, 3, 1, 2, 2, 3, 3, 3, 1, 2, 1, 2, 1, 2, 3, 1, 2, 2, 1, 2, 3, 2, 2, 1, 2, 3, 3, 3, 3], \"Freq\": [0.9882593909761463, 0.8897969545704911, 0.6751346853487857, 0.32289050168854966, 0.9641156004041878, 0.10261294978378895, 0.9235165480541006, 0.9240168533033817, 0.16636602191746883, 0.8318301095873442, 0.9813177133368134, 0.8503803060900401, 0.11729383532276416, 0.9449807641578104, 0.953481423108847, 0.956937755507025, 0.05036514502668552, 0.9468263753995916, 0.06763045538568511, 0.9449807641578104, 0.4422675040193113, 0.5307210048231735, 0.9601624223493703, 0.2022917524266486, 0.7866901483258556, 0.022476861380738733, 0.03814952633672182, 0.9537381584180455, 0.2938062014744579, 0.7345155036861448, 0.5286800909991304, 0.48462341674920284, 0.9235998202179593, 0.9925665175235877, 0.9109487366379129, 0.09423607620392202, 0.9974907594858924, 0.9873072831043964, 0.9691995226166438, 0.7310053636315123, 0.25800189304641613, 0.9670956352639797, 0.9991113093416637, 0.9705302759744164, 0.9493128728201096, 0.9595257809646198, 0.9855501941999862, 0.9517750667478536, 0.9633531730901217, 0.994848542106459, 0.981304673925504, 0.15346906194261276, 0.805712575198717, 0.9885512019654027, 0.626042647905208, 0.2987930819547584, 0.07114120998922818, 0.210082972394332, 0.7703042321125506, 0.37918352799808147, 0.10111560746615506, 0.530856939197314, 0.29635807045635476, 0.673541069218988, 0.6546966733802073, 0.2946135030210933, 0.04910225050351555, 0.8895901653039544, 0.9766599762103608, 0.9908163559895162, 0.7732849086169118, 0.20970438199780658, 0.016383154843578638, 0.970526526723317, 0.9668552300158652, 0.9791009479324675, 0.9278891853771186, 0.4478671076930882, 0.16123215876951175, 0.3762083704621941, 0.9985675231558453, 0.9535401080098063, 0.9895860489391173, 0.9813139779009064, 0.9840671707605708, 0.9823834147568152, 0.969075696563578, 0.7602651510169546, 0.23079477798728978, 0.013576163411017047, 0.1893332038965781, 0.7573328155863124, 0.6626153159500101, 0.31903700397593077, 0.9635931842639665, 0.312484154583593, 0.5570369712142311, 0.13586267590591003, 0.2412578155354456, 0.7720250097134259, 0.9535798730768678, 0.2294182921558551, 0.17206371911689133, 0.5735457303896377, 0.9670619270310568, 0.2832340385146922, 0.7238203206486579, 0.983453060340505, 0.331834475797544, 0.663668951595088, 0.9464764559607621, 0.9704877591361882, 0.6344198109603222, 0.3524554505335123, 0.9968980779462372, 0.9640647993872898, 0.953450424914266, 0.04145436630062026, 0.7223241110586696, 0.27287799751105296, 0.9639799452519248, 0.9516047706503895, 0.4204921629603809, 0.5850325745535734, 0.9535316606431937, 0.08164161752208825, 0.8980577927429708, 0.9852597893108053, 0.015394684207981332, 0.8897343627771835, 0.6393604156687251, 0.16924246297113313, 0.1880471810790368, 0.07024157398025613, 0.9131404617433295, 0.970474491208606, 0.8897176992801378, 0.40723749774953677, 0.5854039030149591, 0.9640050058354461, 0.810929365173257, 0.16695604577096468, 0.9982068145720451, 0.9812877407310795, 0.17364161283268478, 0.8103275265525289, 0.7755690432614308, 0.21543584535039745, 0.9759930883227653, 0.31854720140996545, 0.10618240046998849, 0.5733849625379378, 0.9790356632374292, 0.26357432249369817, 0.1581445934962189, 0.579863509486136, 0.9641082610929107, 0.08432779427833939, 0.8854418399225636, 0.042163897139169695, 0.9499136788060489, 0.05587727522388523, 0.976748904186936, 0.9636191376822775, 0.9778284836441486, 0.996901547801551, 0.9705233313340573, 0.9771460365671271, 0.0279184581876322, 0.7761031230260264, 0.22174374943600753, 0.9644783574930836, 0.9641082506244335, 0.9179009738024687, 0.9991111008601646, 0.9931496919545502, 0.9344049618590476, 0.04672024809295239, 0.17170491950172878, 0.8070131216581253, 0.034340983900345755, 0.9827069350929164, 0.9236023112289738, 0.9813144317614994, 0.9705225664961921, 0.8897969545704911, 0.9873276192181485, 0.040864845115155146, 0.9398914376485684, 0.9886043403664628, 0.9812190423464848, 0.5592156018244883, 0.3219223345814562, 0.11781693688290407, 0.884124026525748, 0.1004686393779259, 0.9237106072093568, 0.5812351059656176, 0.35014163009976956, 0.06302549341795853, 0.923618427878643, 0.9925764068278128, 0.963994268717721, 0.9036753031316238, 0.09036753031316237, 0.8896930429501514, 0.8897969545704911, 0.9704919449202307], \"Term\": [\"also\", \"anyways\", \"appreciated\", \"appreciated\", \"arms\", \"arrested\", \"arrested\", \"asks\", \"attack\", \"attack\", \"attend\", \"back\", \"back\", \"bcs\", \"beautiful\", \"bengal\", \"bengal\", \"best\", \"best\", \"bh\", \"birthday\", \"birthday\", \"bjp\", \"ca\", \"ca\", \"ca\", \"case\", \"case\", \"chief\", \"chief\", \"cm\", \"cm\", \"community\", \"connect\", \"could\", \"could\", \"court\", \"crying\", \"damn\", \"day\", \"day\", \"definitely\", \"dm\", \"driver\", \"dying\", \"engaged\", \"epaper\", \"feel\", \"first\", \"follow\", \"followers\", \"fun\", \"fun\", \"genie\", \"get\", \"get\", \"get\", \"gets\", \"gets\", \"go\", \"go\", \"go\", \"going\", \"going\", \"good\", \"good\", \"good\", \"governor\", \"govt\", \"great\", \"happy\", \"happy\", \"happy\", \"hc\", \"hey\", \"holding\", \"home\", \"india\", \"india\", \"india\", \"joy\", \"judge\", \"keep\", \"killed\", \"kisses\", \"koalas\", \"let\", \"like\", \"like\", \"like\", \"live\", \"live\", \"ll\", \"ll\", \"long\", \"love\", \"love\", \"love\", \"make\", \"make\", \"mcd\", \"meet\", \"meet\", \"meet\", \"members\", \"minister\", \"minister\", \"miss\", \"modi\", \"modi\", \"morning\", \"mp\", \"much\", \"much\", \"mutual\", \"nayar\", \"never\", \"never\", \"new\", \"new\", \"number\", \"old\", \"one\", \"one\", \"others\", \"party\", \"party\", \"people\", \"people\", \"plea\", \"please\", \"please\", \"please\", \"police\", \"police\", \"pradesh\", \"probably\", \"re\", \"re\", \"reading\", \"really\", \"really\", \"recent\", \"sad\", \"said\", \"said\", \"says\", \"says\", \"school\", \"see\", \"see\", \"see\", \"sent\", \"since\", \"since\", \"since\", \"sincerely\", \"smile\", \"smile\", \"smile\", \"sorry\", \"sorry\", \"still\", \"stream\", \"supreme\", \"sure\", \"tells\", \"thank\", \"thank\", \"thanks\", \"thanks\", \"thefashionicon\", \"theres\", \"things\", \"thirst\", \"thursday\", \"time\", \"time\", \"today\", \"today\", \"today\", \"tomorrow\", \"tonight\", \"top\", \"trial\", \"trophy\", \"trump\", \"try\", \"try\", \"tweets\", \"twitter\", \"unhappy\", \"unhappy\", \"unhappy\", \"us\", \"us\", \"vote\", \"want\", \"want\", \"want\", \"wants\", \"week\", \"yeah\", \"year\", \"year\", \"years\", \"yg\", \"york\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el708256327843047316624350\", ldavis_el708256327843047316624350_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el708256327843047316624350\", ldavis_el708256327843047316624350_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el708256327843047316624350\", ldavis_el708256327843047316624350_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0     -0.108309  0.104259       1        1  39.711651\n",
       "1     -0.055724 -0.129207       2        1  33.064962\n",
       "2      0.164033  0.024948       3        1  27.223387, topic_info=        Term        Freq       Total Category  logprob  loglift\n",
       "2553   happy  610.000000  610.000000  Default  30.0000  30.0000\n",
       "2459   great  105.000000  105.000000  Default  29.0000  29.0000\n",
       "2218  follow  100.000000  100.000000  Default  28.0000  28.0000\n",
       "370     also   71.000000   71.000000  Default  27.0000  27.0000\n",
       "4545  recent   77.000000   77.000000  Default  26.0000  26.0000\n",
       "...      ...         ...         ...      ...      ...      ...\n",
       "3321      ll   13.122423   40.747624   Topic3  -6.0592   0.1680\n",
       "5103   since   10.713432   18.969981   Topic3  -6.2620   0.7297\n",
       "1218      cm   10.784856   22.698036   Topic3  -6.2554   0.5570\n",
       "3528    meet   10.391169   17.435401   Topic3  -6.2925   0.7835\n",
       "2553   happy   10.131243  610.383049   Topic3  -6.3179  -2.7974\n",
       "\n",
       "[181 rows x 6 columns], token_table=      Topic      Freq         Term\n",
       "term                              \n",
       "370       2  0.988259         also\n",
       "436       3  0.889797      anyways\n",
       "467       1  0.675135  appreciated\n",
       "467       2  0.322891  appreciated\n",
       "483       2  0.964116         arms\n",
       "...     ...       ...          ...\n",
       "6269      2  0.903675         year\n",
       "6269      3  0.090368         year\n",
       "6270      3  0.889693        years\n",
       "6280      3  0.889797           yg\n",
       "6290      3  0.970492         york\n",
       "\n",
       "[208 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2, 3])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(p3, 'lda3.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3En3pqNaNdO"
   },
   "source": [
    "Bonuses\n",
    "- achieve a better accuracy on the test dataset – 0.851\n",
    "- achieve an even better accuracy on the test dataset – 0.873\n",
    "- use word2vec for vectorization of texts\n",
    "- detailed classification_report\n",
    "- more preprocessing ways (8 instead of 6)\n",
    "- LDA\n",
    "- some visualization"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VKiVE93uu16j",
    "ckN2SrfezY89",
    "l1av-jRdzivW"
   ],
   "name": "tweets26/02.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}